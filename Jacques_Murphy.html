<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Julien Jacques">
<meta name="author" content="Thomas Brendan Murphy">
<meta name="dcterms.date" content="2025-03-18">
<meta name="keywords" content="Count data, Model-based clustering, Variable selection">

<title>Model-Based Clustering and Variable Selection for Multivariate Count Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Jacques_Murphy_files/libs/clipboard/clipboard.min.js"></script>
<script src="Jacques_Murphy_files/libs/quarto-html/quarto.js"></script>
<script src="Jacques_Murphy_files/libs/quarto-html/popper.min.js"></script>
<script src="Jacques_Murphy_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Jacques_Murphy_files/libs/quarto-html/anchor.min.js"></script>
<link href="Jacques_Murphy_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Jacques_Murphy_files/libs/quarto-html/quarto-syntax-highlighting-de84f8d6bb715db06a919283c2d1e787.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Jacques_Murphy_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Jacques_Murphy_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Jacques_Murphy_files/libs/bootstrap/bootstrap-5f1fb0355f74790845d18183fcf83f78.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="Jacques_Murphy_files/libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="Jacques_Murphy_files/libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>
<meta name="quarto:status" content="draft">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Model-Based Clustering and Variable Selection for Multivariate Count Data</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        Julien Jacques 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Laboratoire ERIC, Université de Lyon
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        Thomas Brendan Murphy 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  School of Mathematics &amp; Statistics, University College Dublin
                </p>
              <p class="affiliation">
                  Institut d’Études Avancées, Université de Lyon
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 18, 2025</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">March 18, 2025</p>
      </div>
    </div>
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Count data, Model-based clustering, Variable selection</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <p class="date">draft</p>
                  </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>Model-based clustering provides a principled way of developing clustering methods. We develop a new model-based clustering methods for count data. The method combines clustering and variable selection for improved clustering. The method is based on conditionally independent Poisson mixture models and Poisson generalized linear models. The method is demonstrated on simulated data and data from an ultra running race, where the method yields excellent clustering and variable selection performance.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#motivating-example" id="toc-motivating-example" class="nav-link" data-scroll-target="#motivating-example"><span class="header-section-number">2</span> Motivating Example</a></li>
  <li><a href="#independent-poisson-mixture" id="toc-independent-poisson-mixture" class="nav-link" data-scroll-target="#independent-poisson-mixture"><span class="header-section-number">3</span> Independent Poisson Mixture</a></li>
  <li><a href="#variable-selection" id="toc-variable-selection" class="nav-link" data-scroll-target="#variable-selection"><span class="header-section-number">4</span> Variable selection</a>
  <ul class="collapse">
  <li><a href="#model-setup" id="toc-model-setup" class="nav-link" data-scroll-target="#model-setup"><span class="header-section-number">4.1</span> Model setup</a></li>
  <li><a href="#sec-interpretation" id="toc-sec-interpretation" class="nav-link" data-scroll-target="#sec-interpretation"><span class="header-section-number">4.2</span> Interpretation</a></li>
  <li><a href="#stepwise-selection-algorithm" id="toc-stepwise-selection-algorithm" class="nav-link" data-scroll-target="#stepwise-selection-algorithm"><span class="header-section-number">4.3</span> Stepwise selection algorithm</a>
  <ul class="collapse">
  <li><a href="#screening-variables-initialization" id="toc-screening-variables-initialization" class="nav-link" data-scroll-target="#screening-variables-initialization"><span class="header-section-number">4.3.1</span> Screening variables: Initialization</a></li>
  <li><a href="#stepwise-algorithm-updating" id="toc-stepwise-algorithm-updating" class="nav-link" data-scroll-target="#stepwise-algorithm-updating"><span class="header-section-number">4.3.2</span> Stepwise algorithm: Updating</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#simulation-study" id="toc-simulation-study" class="nav-link" data-scroll-target="#simulation-study"><span class="header-section-number">5</span> Simulation study</a>
  <ul class="collapse">
  <li><a href="#illustrative-example" id="toc-illustrative-example" class="nav-link" data-scroll-target="#illustrative-example"><span class="header-section-number">5.1</span> Illustrative example</a></li>
  <li><a href="#sec-Scenari" id="toc-sec-Scenari" class="nav-link" data-scroll-target="#sec-Scenari"><span class="header-section-number">5.2</span> Scenarios of simulation</a></li>
  <li><a href="#sec-wholeresults" id="toc-sec-wholeresults" class="nav-link" data-scroll-target="#sec-wholeresults"><span class="header-section-number">5.3</span> Results</a></li>
  </ul></li>
  <li><a href="#international-ultrarunning-association-data" id="toc-international-ultrarunning-association-data" class="nav-link" data-scroll-target="#international-ultrarunning-association-data"><span class="header-section-number">6</span> International Ultrarunning Association Data</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">7</span> Discussion</a></li>
  <li><a href="#acknowlegements" id="toc-acknowlegements" class="nav-link" data-scroll-target="#acknowlegements"><span class="header-section-number">8</span> Acknowlegements</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="Jacques_Murphy.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Multivariate count data is ubiquitous in statistical applications, as ecology <span class="citation" data-cites="Chiquet2021">(<a href="#ref-Chiquet2021" role="doc-biblioref">Chiquet, Mariadassou, and Robin 2021</a>)</span>, genomics <span class="citation" data-cites="Rau2015 Silva2019">(<a href="#ref-Rau2015" role="doc-biblioref">Rau et al. 2015</a>; <a href="#ref-Silva2019" role="doc-biblioref">Silva et al. 2019</a>)</span>. These data arise when each observation consists of a vector of count values. Count data are often treated as continuous data and therefore modeled by a Gaussian distribution, this assumption is particularly poor when the measured counts are low. Instead, we use the reference distribution for count data which is the Poisson distribution <span class="citation" data-cites="Agresti_2002 Inouye1998">(<a href="#ref-Agresti_2002" role="doc-biblioref">Agresti 2013</a>; <a href="#ref-Inouye1998" role="doc-biblioref">Inouye et al. 2017a</a>)</span>.</p>
<p>When a data set is heterogeneous, clustering allows to extract homogeneous subsets from the whole data set. Many clustering methods, such as <span class="math inline">k</span>-means <span class="citation" data-cites="Hartigan_1979">(<a href="#ref-Hartigan_1979" role="doc-biblioref">Hartigan and Wong 1979</a>)</span>, are geometric in nature, whereas many modern clustering approaches are based on probabilistic models. In this work, we use model-based clustering which has been developed for many types of data <span class="citation" data-cites="Bouveyron_2019 McLachlanPeel2000 Fruhwirth_2018">(<a href="#ref-Bouveyron_2019" role="doc-biblioref">Bouveyron et al. 2019</a>; <a href="#ref-McLachlanPeel2000" role="doc-biblioref">McLachlan and Peel 2000</a>; <a href="#ref-Fruhwirth_2018" role="doc-biblioref">Frühwirth-Schnatter, Celeux, and Robert 2018</a>)</span>.</p>
<p>Modern data are often high-dimensional, that is the number of variables is often large. Among these variables, some are useful for the task of interest, some are useless for the task of interest and some others are useful but redundant. There is a need to select only the relevant variables, and that whatever is the task. Variable selection methods are widespread for supervised learning tasks, in particular to avoid overfitting. However, variable selection methods are less well developed for unsupervised learning tasks, such as clustering. Recently, several methods have been proposed for selecting the relevant variables in model-based clustering; we refer to <span class="citation" data-cites="Fop_2018">Fop and Murphy (<a href="#ref-Fop_2018" role="doc-biblioref">2018</a>)</span> and <span class="citation" data-cites="McParlandMurphy2018">McParland and Murphy (<a href="#ref-McParlandMurphy2018" role="doc-biblioref">2018</a>)</span> for recent detailed surveys.</p>
<p>The goal of the present work is to provide a clustering and variable selection method for multivariate count data, which, to the best of our knowledge, has not yet been studied in depth. A methodology based on a conditionally independent Poisson mixture is developed to achieve this goal. The method yields a final clustering model which is a conditionally independent Poisson mixture model for a subset of the variables.</p>
</section>
<section id="motivating-example" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Motivating Example</h1>
<p>The International Association of Ultrarunners (IAU) 24 hour World Championships were held in Katowice, Poland from September 8th to 9th, 2012. Two hundred and sixty athletes representing twenty four countries entered the race, which was held on a course consisting of a 1.554 km looped route. An update of the number of laps covered by each athlete was recorded approximately every hour <span class="citation" data-cites="WhiteMurphy2016">(<a href="#ref-WhiteMurphy2016" role="doc-biblioref">White and Murphy 2016</a>)</span>. <a href="#fig-24H" class="quarto-xref">Figure&nbsp;1</a> plots the number of loops recorded each hour for the three medalists.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-24H" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-24H-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-24H-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-24H-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Number of loops per hour for the three medalists.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see among these three runners different strategies, the second placed runner lapped at a regular rate, the first placed runner had a fast start but slowed later, and the third placed runner also started fast but slowed more than the first place runner.</p>
<p>Our first goal will be, to analyze the whole data set to identify the different running strategies and to evaluate which strategies are the best ones. The second goal is to identify which variables allows to distinguish between the clusters, in order to identify which hour is essential in the management of this endurance race.</p>
</section>
<section id="independent-poisson-mixture" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Independent Poisson Mixture</h1>
<p>Let <span class="math inline">X_n=(X_{n1}, X_{n2}, \ldots, X_{nM})</span> be a random vector of counts for <span class="math inline">n=1, 2, \ldots, N</span>. The goal is to clusters theses <span class="math inline">N</span> observations into <span class="math inline">G</span> clusters. Let <span class="math inline">Z_n=(Z_{n1}, Z_{n2}, \ldots, Z_{nG})</span> be the latent cluster indicator vector, where <span class="math inline">Z_{ng}=1</span> if observation <span class="math inline">n</span> belongs to cluster <span class="math inline">g</span> and <span class="math inline">Z_{ng}=0</span> otherwise. We assume that <span class="math inline">{\mathbb P}\{Z_{ng}=1\} = \tau_g</span> for <span class="math inline">g=1,2, \ldots, G</span>. Let denote <span class="math inline">\tau=(\tau_1,\ldots,\tau_G)</span>. The conditionally independent Poisson mixture model <span class="citation" data-cites="Karlis2018">(<a href="#ref-Karlis2018" role="doc-biblioref">Karlis 2018, sec. 9.4.2.1</a>)</span> assumes that the elements of <span class="math inline">X_n</span> are independent Poisson distributed random variables, conditional on <span class="math inline">Z_n</span>. That is, <span class="math display">
\begin{aligned}
Z_n &amp; \sim  {\rm Multinomial}(1, \tau)\\
X_{nm}|(Z_{ng}=1) &amp; \sim  {\rm Poisson}(\lambda_{gm}), {\rm ~for~} m=1, 2, \ldots, M.
\end{aligned}
</span> Alternative modelling frameworks exist, either to introduce some dependence between variables or to normalize the variables. We refer the interested reader to <span class="citation" data-cites="Karlis2018 Bouveyron_2019">(<a href="#ref-Karlis2018" role="doc-biblioref">Karlis 2018</a>; <a href="#ref-Bouveyron_2019" role="doc-biblioref">Bouveyron et al. 2019, chap. 6</a>)</span> for more details.</p>
<p>Denoting the model parameters by <span class="math inline">\theta=(\tau,\lambda)</span> where <span class="math inline">\lambda=(\lambda_{gm})_{1\leq g \leq G, 1\leq m \leq M}</span>, and where <span class="math inline">X=(x_n)_{1\leq n \leq N}</span> denotes the observations, the observed likelihood is <span class="math display">
L(\theta)=\sum_{n=1}^{N}\sum_{g=1}^{G}\tau_g\prod_{m=1}^M\phi(x_{nm},\lambda_{gm}),
</span> where <span class="math inline">\phi(x,\lambda)=\exp(-\lambda)\lambda^{x}/x!</span>, the Poisson probability mass function.</p>
<p>Due to form of the mixture distribution, there are no closed form for the maximum likelihood estimators, and an iterative EM algorithm needs to be used <span class="citation" data-cites="Dempster_1977">(<a href="#ref-Dempster_1977" role="doc-biblioref">Dempster, Laird, and Rubin 1977</a>)</span> to maximize the likelihood. The EM algorithm consists, starts from an initial value <span class="math inline">\theta^{(0)}</span> for the model parameter, and alternates the two following steps until convergence of the likelihood.</p>
<p>At the <span class="math inline">q</span>th iteration of the EM algorithm, the E-step consists of computing for all <span class="math inline">1\leq n\leq N</span> and <span class="math inline">1\leq g\leq G</span>: <span class="math display">t_{ng}^{(q)}=\frac{\tau_g^{(q)}\prod_{m=1}^M\phi(x_{nm},\lambda_{gm})}{\sum_{h=1}^{G}\tau_h^{(q)}\prod_{m=1}^M\phi(x_{nm},\lambda_{hm})}.</span> In the M-step, the model parameters are updated as follows: <span class="math display"> \tau_g^{(q+1)}=\frac{\sum_{n=1}^Nt_{ng}^{(q)}}{N}
\quad {\rm ~and~} \quad
\lambda_{gm}^{(q+1)}=\frac{\sum_{n=1}^Nt_{ng}^{(q)}x_{nm}}{\sum_{n=1}^Nt_{ng}^{(q)}}.
</span> The EM algorithm steps are iterated until convergence, where convergence is determined when <span class="math inline">\log L(\theta^{(q+1)}) - \log L(\theta^{(q)}) &lt; \epsilon</span>.</p>
<p>The number of clusters <span class="math inline">G</span> is selected using the Bayesian information criterion (BIC) <span class="citation" data-cites="Schwarz_1978">(<a href="#ref-Schwarz_1978" role="doc-biblioref">Schwarz 1978</a>)</span>, <span class="math display">
BIC= 2 \log L(\hat\theta) - \{(G - 1) + GM\}\log(N),
</span> where <span class="math inline">\hat\theta</span> is the maximum likelihood estimate of the model parameters; models with higher BIC are prefered to models with lower BIC.</p>
</section>
<section id="variable-selection" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Variable selection</h1>
<p>We develop a model-based clustering method with variable selection for multivariate count data. The method follows the approach of <span class="citation" data-cites="RafteryDean2006 MaugisEtAl2009">(<a href="#ref-RafteryDean2006" role="doc-biblioref">Raftery and Dean 2006</a>; <a href="#ref-MaugisEtAl2009" role="doc-biblioref">Maugis, Celeux, and Martin-Magniette 2009</a>)</span> for continuous data and <span class="citation" data-cites="DeanRaftery2010 Fop_2017">(<a href="#ref-DeanRaftery2010" role="doc-biblioref">Dean and Raftery 2010</a>; <a href="#ref-Fop_2017" role="doc-biblioref">Fop, Smart, and Murphy 2017</a>)</span> for categorical data. It consists in a stepwise model comparison approach where variables are added and removed from a set of clustering variables.</p>
<section id="model-setup" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="model-setup"><span class="header-section-number">4.1</span> Model setup</h2>
<p>The clustering and variable selection approach is based around partitioning <span class="math inline">{X}_n=(X_n^{C}, X_n^{P}, X_n^{O})</span> into three parts:</p>
<ul>
<li><span class="math inline">X_n^{C}</span>: The current clustering variables,</li>
<li><span class="math inline">X_n^{P}</span>: The proposed variable to add to the clustering variables,</li>
<li><span class="math inline">X_n^{O}</span>: The other variables.</li>
</ul>
<p>For simplicity of notation, <span class="math inline">C</span> will be used to denote the set of indices of the current clustering variables, <span class="math inline">P</span> the indices of the proposed variable and <span class="math inline">O</span> the indices of the other one. Then <span class="math inline">(C,P,O)</span> is a partition of <span class="math inline">\{1,\ldots,M\}</span>.</p>
<p>The decision on whether to add the proposed variable to the clustering variables is based on comparing two models:</p>
<p><span class="math inline">{\cal M}_1</span> (Clustering Model), which assumes that the proposed variable is useful for clustering: <span class="math display">
(X_n^C, X_n^P) \sim \sum_{g=1}^{G} \tau_g \prod_{m\in\{C, P\}} {\rm Poisson}(\lambda_{gm}).
</span> The <span class="math inline">{\cal M}_1</span> model is fitted for different values of <span class="math inline">G</span> between <span class="math inline">1</span> and <span class="math inline">G_{max}</span> to achieve the best clustering model.</p>
<p><span class="math inline">{\cal M}_2</span> (Non-Clustering Model) which assumes that the proposed variable is not useful for clustering, but is potentially linked to the clustering variables through a Poisson GLM, that is, <span class="math display">
\begin{aligned}
X_n^C &amp;\sim \sum_{g=1}^{G}\tau_g \prod_{m\in C} {\rm Poisson}(\lambda_{gm})\\
X_n^P| (X_n^C=x_n^C, Z_{ng}=1) &amp; \sim  {\rm PoissonGLM}(x_n^{(C)}),
\end{aligned}
</span> where Poisson GLM states that <span class="math display">
\log {\mathbb E}[X_n^P|X_n^C=x_n^C, Z_{ng}=1] = \alpha + \beta^\top x_n^C.
</span></p>
<p>In order to avoid non significant terms in the Poisson GLM model, a standard stepwise variable selection approach (using BIC as the variable selection criterion) is considered. Thus, the proposed variable <span class="math inline">X_n^P</span> will be dependent on only a subset <span class="math inline">X_n^R</span> of the clustering variables <span class="math inline">X_n^C</span>. We note that <span class="math inline">G</span> is fixed in the non-clustering model, because an optimal value for <span class="math inline">G</span> is previously chosen.</p>
<p>The clustering and non-clustering models are represented as graphical models in <a href="#fig-graphical" class="quarto-xref">Figure&nbsp;2</a>.</p>
<div id="fig-graphical" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graphical-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 80.0%;justify-content: flex-start;">
<div class="sourceCode" id="cb1"><pre class="sourceCode tikz code-with-copy"><code class="sourceCode latex"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">%%| filename: ../figures/fig-graphical</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">\begin</span>{<span class="ex">tikzpicture</span>}</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[draw, circle] at (0, 0) (z) {<span class="ss">$Z$</span>};</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">\node</span>[draw] at (-2, -2) (xc) {<span class="ss">$X^C$</span>};</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[draw] at (2, -2) (xp) {<span class="ss">$X^P$</span>};</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[draw] at (0, -4) (xo) {<span class="ss">$X^O$</span>};</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">\draw</span>[-&gt;] (z) -- (xp);</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">\draw</span>[-&gt;] (z) -- (xc);</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">\draw</span>[-&gt;] (xc) -- (xo);</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">\draw</span>[-&gt;] (xp) -- (xo);</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">\node</span>[draw, rectangle] at (0, 0.75) {Clustering};</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[draw, circle] at (7, 0) (z) {<span class="ss">$Z$</span>};</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">\node</span>[draw] at (5, -2) (xc) {<span class="ss">$X^C$</span>};</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[draw] at (9, -2) (xp) {<span class="ss">$X^P$</span>};</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[draw] at (7, -4) (xo) {<span class="ss">$X^O$</span>};</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a> <span class="fu">\draw</span>[-&gt;] (z) -- (xc);</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a> <span class="fu">\draw</span>[dashed,-&gt;] (xc) -- (xp);</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a> <span class="fu">\node</span>[fill=white] at (7, -1.7) {<span class="ss">$X^R</span><span class="sc">\subseteq</span><span class="ss"> X^C$</span>};</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">\draw</span>[-&gt;] (xc) -- (xo);</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">\draw</span>[-&gt;] (xp) -- (xo);</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">\node</span>[draw, rectangle] at (7, 0.75) {Non Clustering};</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">\end</span>{<span class="ex">tikzpicture</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graphical-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Graphical model representations of the clustering and non-clustering models.
</figcaption>
</figure>
</div>
<p>Thus, there is two reasons for which <span class="math inline">{\cal M}_2</span> can be preferred to <span class="math inline">{\cal M}_1</span>: either <span class="math inline">X_n^P</span> does not contain information about the latent clustering variable at all (ie. <span class="math inline">X_n^R=\emptyset</span>), or <span class="math inline">X_n^P</span> does not add further useful information about the clustering given the information already contained in the current clustering variables. In the first situation, we say that <span class="math inline">X_n^P</span> is an irrelevant variable, because it contains no clustering information. In the second situation, we say that <span class="math inline">X_n^P</span> is a redundant variable because it contains no extra information about the clustering beyond the current clustering variables (<span class="math inline">X_n^C</span>).</p>
<p>Additionally, both models assume the same form for the conditional distribution for <span class="math inline">X_n^{O}|(X_{n}^{C}, X_{n}^{P})</span> and whose form doesn’t need to be explicitly specified because it doesn’t affect the model choice.</p>
<p>Variable <span class="math inline">P</span> is added to <span class="math inline">C</span> if the clustering model (<span class="math inline">{\cal M}_1</span>) is preferred to the non-clustering model (<span class="math inline">{\cal M}_2</span>). In order to compare <span class="math inline">{\cal M}_1</span> and <span class="math inline">{\cal M}_2</span>, following <span class="citation" data-cites="DeanRaftery2010">(<a href="#ref-DeanRaftery2010" role="doc-biblioref">Dean and Raftery 2010</a>)</span>, we consider the Bayes Factor: <span class="math display">B_{1,2}=\frac{p(X|{\cal M}_1)}{p(X|{\cal M}_2)}</span> which is asymptotically approximated <span class="citation" data-cites="Fop_2017 Kass_1995">(<a href="#ref-Fop_2017" role="doc-biblioref">Fop, Smart, and Murphy 2017</a>; <a href="#ref-Kass_1995" role="doc-biblioref">Kass and Raftery 1995</a>)</span> using the difference of the BIC criteria for both models: <span class="math display">2\log B_{1,2}\simeq BIC_{{\cal M}_1}-BIC_{{\cal M}_2}.</span></p>
<p>The same modelling framework can be used for removing variables from the current set of clustering variables.</p>
</section>
<section id="sec-interpretation" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-interpretation"><span class="header-section-number">4.2</span> Interpretation</h2>
<p>Comparing <span class="math inline">{\cal M}_1</span> and <span class="math inline">{\cal M}_2</span> is equivalent to comparing the following <span class="math inline">X_n^P|(X_n^C=x_n^C)</span> structures.</p>
<p>The <span class="math inline">{\cal M}_1</span> (Clustering Model) assumes that, <span class="math display">
X_n^P| (X_n^C=x_n^C) \sim \sum_{g=1}^{G} {\mathbb P}\{Z_{ng}=1|X_{n}^{C}=x_{n}^{C}\} {\rm Poisson}(\lambda_{gm}),
</span> where <span class="math display">
{\mathbb P}\{Z_{ng}=1|X_{n}^{C}=x_{n}^{C}\} = \frac{\tau_g \prod_{m=1}^{M}\phi(x_{nm}, \lambda_{gm})}{\sum_{h=1}^{G}\tau_h \prod_{m=1}^{M}\phi(x_{nm}, \lambda_{hm})}.
</span></p>
<p>Whereas, the <span class="math inline">{\cal M}_2</span> (Non-Clustering Model) assumes that, <span class="math display">
X_n^P| (X_n^C=x_n^C) =  {\rm PoissonGLM}(x_n^C).
</span></p>
<p>The method contrasts which of conditional model structures is better describing the distribution of the proposed variable <span class="math inline">X^P</span>. The clustering model (<span class="math inline">{\cal M}_1</span>) uses a mixture model, with covariate dependent weights, for the conditional model whereas the non-clustering model (<span class="math inline">{\cal M}_2</span>) is a Poisson generalized linear model. The model selection criterion chooses the model that best models this conditional distribution.</p>
</section>
<section id="stepwise-selection-algorithm" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="stepwise-selection-algorithm"><span class="header-section-number">4.3</span> Stepwise selection algorithm</h2>
<section id="screening-variables-initialization" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="screening-variables-initialization"><span class="header-section-number">4.3.1</span> Screening variables: Initialization</h3>
<p>We start with an initial choice of <span class="math inline">C</span> by first screening each individual variable by fitting a mixture of univariate Poisson distributions <span class="citation" data-cites="EverittHand1981">(eg. <a href="#ref-EverittHand1981" role="doc-biblioref">Everitt and Hand 1981, chap. 4.3</a>)</span>, <span class="math display">
X_{nm} \sim \sum_{g=1}^{G}\tau_g {\rm Poisson}(\lambda_{gm}), {\rm ~for~} G=1,2,\ldots, G_{max}.
</span> The initial set of variables is set to be those variables where any model with <span class="math inline">G&gt;1</span> is preferred to the <span class="math inline">G=1</span> model.</p>
</section>
<section id="stepwise-algorithm-updating" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="stepwise-algorithm-updating"><span class="header-section-number">4.3.2</span> Stepwise algorithm: Updating</h3>
<p>We consider a stepwise algorithm which alternates between adding and removing steps. In the removal step, all the variables in <span class="math inline">X^C</span> are examined in turn to be removed from the set. In the adding step, all the variables in <span class="math inline">X^O</span> are examined in turn to be added to the clustering set.</p>
<p>The algorithm also performs the selection of the number <span class="math inline">G</span> of clusters finding at each stage the optimal combination of clustering variables and number of clusters. The procedure stops when no change has been made to the set <span class="math inline">X^C</span> after consecutive exclusion and inclusion steps.</p>
<p>With the present stepwise selection algorithm, it can occur that during the process, we get back on a solution (a set of clustering variable) already explored. Since our algorithm is not stochastic, we fall into an infinite cycle. In this situation the algorithm is stopped, and the best solution according to BIC among the solution of the cycle is kept.</p>
</section>
</section>
</section>
<section id="simulation-study" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Simulation study</h1>
<p>In this section, we evaluate the proposed variable selection method through three different simulation scenarios. We start with an illustrative example in which, using a data set simulated according to the proposed model, we show how to perform the variable selection.</p>
<p>Then, simulation studies are performed to evaluate the behavior of the proposed selection method, when the data are simulated according to the proposed model (Scenario1) and when the model assumptions are violated. In Scenario2, the link between <span class="math inline">X^R</span> and <span class="math inline">X^C</span> is no longer a Poisson GLM but a linear model. In Scenario3, the clustering variables are no longer conditionally independent.</p>
<section id="illustrative-example" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="illustrative-example"><span class="header-section-number">5.1</span> Illustrative example</h2>
<p>In the first simulation setting we consider 10 Poisson random variables. Variables <span class="math inline">X_1</span>, <span class="math inline">X_2</span>, <span class="math inline">X_3</span> and <span class="math inline">X_4</span> are the clustering variables, distributed according to a mixture of <span class="math inline">G=</span> 3 independent Poisson mixture distributions with mixing proportions 0.4, 0.3, 0.3. Variables <span class="math inline">X_5</span>, <span class="math inline">X_6</span> and <span class="math inline">X_7</span> are redundant variables, each one generated dependent on the clustering variables. These three variables are linked to the four first ones through a Poisson GLM. The last three variables, <span class="math inline">X_8</span>, <span class="math inline">X_9</span> and <span class="math inline">X_{10}</span> are irrelevant variables not related to the previous ones. <a href="#tbl-ParamSimul" class="quarto-xref">Table&nbsp;1</a> shows the parameter of the Poisson distribution for each variable and each cluster.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-ParamSimul" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ParamSimul-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: True values of component parameters (Scenario 1)
</figcaption>
<div aria-describedby="tbl-ParamSimul-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 3%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g1}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g2}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g3}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g4}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g5}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g6}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g7}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g8}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g9}</span></th>
<th style="text-align: left;"><span class="math inline">\lambda_{g10}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">g=1</span></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="math inline">\lambda_{g5}</span></td>
<td style="text-align: left;"><span class="math inline">\lambda_{g6}</span></td>
<td style="text-align: left;"><span class="math inline">\lambda_{g7}</span></td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">g=2</span></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;"><span class="math inline">\lambda_{g5}</span></td>
<td style="text-align: left;"><span class="math inline">\lambda_{g6}</span></td>
<td style="text-align: left;"><span class="math inline">\lambda_{g7}</span></td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">g=3</span></td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;"><span class="math inline">\lambda_{g5}</span></td>
<td style="text-align: left;"><span class="math inline">\lambda_{g6}</span></td>
<td style="text-align: left;"><span class="math inline">\lambda_{g7}</span></td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>with <span class="math inline">\lambda_{g5}=\exp(0.2X_2)</span>, <span class="math inline">\lambda_{g6}=\exp(0.2X_1-0.1X_2)</span> and <span class="math inline">\lambda_{g7}=\exp(0.1 (X_1+X_3+X_4))</span>.</p>
<p>Below is the result obtained for one data set of size <span class="math inline">N=</span> 400. The evaluation criteria is the selected features (true one are <span class="math inline">X_1</span> to <span class="math inline">X_4</span>) and the Adjusted Rand Index <span class="citation" data-cites="Rand_1971 HubertArabie1985">(<a href="#ref-Rand_1971" role="doc-biblioref">Rand 1971</a>; <a href="#ref-HubertArabie1985" role="doc-biblioref">Hubert and Arable 1985</a>)</span> obtained with the selected variables in comparison to those obtained with the full set of variables and with the true clustering variables.</p>
<p>The independent Poisson mixture model was fitted to the simulated data with <span class="math inline">N=</span> 400 rows and <span class="math inline">P=</span> 10 columns. Models with <span class="math inline">G=1</span> to <span class="math inline">G=</span> 10 were fitted using the EM algorithm.</p>
<p>The values of BIC for the independent Poisson mixture model are plotted in <a href="#fig-BIC" class="quarto-xref">Figure&nbsp;3</a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-BIC" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-BIC-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-BIC-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-BIC-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Bayesian Information Criterion (BIC) for the independent Poisson mixture model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The model with the highest BIC has <span class="math inline">G=</span> 3 components and the resulting estimates of <span class="math inline">\tau</span> and <span class="math inline">\lambda</span> are given as:</p>
<div class="cell">
<div id="tbl-params" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-params-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Estimates of the mixing proportions and component parameters.
</figcaption>
<div aria-describedby="tbl-params-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 3%">
<col style="width: 5%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\tau_g</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g1}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g2}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g3}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g4}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g5}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g6}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g7}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g8}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g9}</span></th>
<th style="text-align: right;"><span class="math inline">\lambda_{g10}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">g=1</span></td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">4.09</td>
<td style="text-align: right;">4.00</td>
<td style="text-align: right;">4.15</td>
<td style="text-align: right;">4.34</td>
<td style="text-align: right;">2.51</td>
<td style="text-align: right;">1.87</td>
<td style="text-align: right;">3.95</td>
<td style="text-align: right;">4.04</td>
<td style="text-align: right;">1.85</td>
<td style="text-align: right;">1.12</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">g=2</span></td>
<td style="text-align: right;">0.42</td>
<td style="text-align: right;">2.04</td>
<td style="text-align: right;">2.11</td>
<td style="text-align: right;">1.34</td>
<td style="text-align: right;">3.74</td>
<td style="text-align: right;">1.64</td>
<td style="text-align: right;">1.27</td>
<td style="text-align: right;">2.00</td>
<td style="text-align: right;">3.91</td>
<td style="text-align: right;">2.06</td>
<td style="text-align: right;">0.96</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">g=3</span></td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">0.93</td>
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">1.08</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">1.13</td>
<td style="text-align: right;">1.01</td>
<td style="text-align: right;">1.16</td>
<td style="text-align: right;">3.82</td>
<td style="text-align: right;">2.02</td>
<td style="text-align: right;">1.00</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>A look at <a href="#tbl-ParamSimul" class="quarto-xref">Table&nbsp;1</a> of true values allows us to say that these estimates are correct (except for label switching).</p>
<p>Let start by initializing the stepwise algorithm.</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>fit_screen <span class="ot">&lt;-</span> <span class="fu">poissonmix_screen</span>(x, <span class="at">G =</span> <span class="dv">1</span><span class="sc">:</span>Gmax)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>jchosen <span class="ot">&lt;-</span> fit_screen<span class="sc">$</span>jchosen</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The variables selected by the screening procedure are {1, 2, 3, 4, 6, 7}.</p>
<p>Now, we execute the stepwise selection algorithm:</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">poissonmix_varsel</span>(x, <span class="at">jchosen=</span>jchosen, <span class="at">G =</span> <span class="dv">1</span><span class="sc">:</span>Gmax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Initial Selected Variables: 1,2,3,4,6,7"
[1] "Iteration: 1"
[1] "Add Variable: NONE 10 BIC Difference: -13.2"
[1] "Remove Variable: 6  BIC Difference: 83.7"
[1] "Current Selected Variables: 1,2,3,4,7"
[1] "Iteration: 2"
[1] "Add Variable: NONE 9 BIC Difference: -10.6"
[1] "Remove Variable: 7  BIC Difference: 50.1"
[1] "Current Selected Variables: 1,2,3,4"
[1] "Iteration: 3"
[1] "Add Variable: NONE 10 BIC Difference: -10.5"
[1] "Remove Variable: NONE 3 BIC Difference: -26.8"
[1] "Current Selected Variables: 1,2,3,4"</code></pre>
</div>
</div>
<p>Note that the computing time is about 5 minutes on a laptop with 2.3 GHz Intel Core i7 processor and 32Go of RAM.</p>
<p>The final chosen variables are {1, 2, 3, 4}.</p>
<p>Finally, the ARI obtained with the selected variables, which turn out to be the true clustering variable, is 0.594 whereas it is 0.432 with all the variables.</p>
</section>
<section id="sec-Scenari" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-Scenari"><span class="header-section-number">5.2</span> Scenarios of simulation</h2>
<p>In this section the three scenario of simulation are described. The first scenario is similar to the previous illustrative example.</p>
<p>The second scenario is similar to the first one, except for variables <span class="math inline">X_5</span>, <span class="math inline">X_6</span> and <span class="math inline">X_7</span> which are still redundant but linked to the true clustering variables through linear, quadratic and exponential term in an identity link function, respectively, and not a Poisson GLM with logarithm link function. More precisely, <span class="math inline">X_5</span>, <span class="math inline">X_6</span> and <span class="math inline">X_7</span> have Poisson distribution of respective parameter <span class="math inline">\lambda_{g5}=\exp(2X_2)</span>, <span class="math inline">\lambda_{g6}=\exp(X_1^2+X_3)</span> and <span class="math inline">\lambda_{g7}=\exp(\exp(0.1 (X_1 + X_3+ X_4)))</span>. Thus, the data are simulated from a model which does not satisfy assumptions of model <span class="math inline">{\cal M}_2</span>.</p>
<p>The third scenario is similar to the second one, but some dependence between the clustering variables <span class="math inline">X_1</span> and <span class="math inline">X_2</span> is introduced, in order to create some redundancy among the true clustering variables. For this, <span class="math inline">X_1</span> and <span class="math inline">X_2</span> are simulated as in the previous setting, and a same term is added to both of these variables (simulated according a Poisson distribution of parameter 2) .</p>
</section>
<section id="sec-wholeresults" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sec-wholeresults"><span class="header-section-number">5.3</span> Results</h2>
<p><a href="#tbl-HISTall" class="quarto-xref">Table&nbsp;3</a> shows the number of times, among the 100 simulated data sets, that each variable is selected. For Scenario 1, the model selection procedure perform perfectly, selecting each time only the true clustering variables. For Scenario 2, due to the fact the link between the redundant and the true clustering variables is not a standard Poisson GLM, the variable selection is perturbed and variables <span class="math inline">X_5</span> is sometimes selected. For Scenario 3, the results is that the dependency between <span class="math inline">X_1</span> and <span class="math inline">X_2</span> perturb the variable selection, and only one of them is selected (and even sometimes none of them). Redundant variables <span class="math inline">X_5</span> and <span class="math inline">X_6</span>, which are linked to the clustering variables but with a linear link, are also sometimes selected.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-HISTall" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HISTall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Number of selection for each variable, simulation setting number 3.
</figcaption>
<div aria-describedby="tbl-HISTall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 11%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">X_{1}</span></th>
<th style="text-align: right;"><span class="math inline">X_{2}</span></th>
<th style="text-align: right;"><span class="math inline">X_{3}</span></th>
<th style="text-align: right;"><span class="math inline">X_{4}</span></th>
<th style="text-align: right;"><span class="math inline">X_{5}</span></th>
<th style="text-align: right;"><span class="math inline">X_{6}</span></th>
<th style="text-align: right;"><span class="math inline">X_{7}</span></th>
<th style="text-align: right;"><span class="math inline">X_{8}</span></th>
<th style="text-align: right;"><span class="math inline">X_{9}</span></th>
<th style="text-align: right;"><span class="math inline">X_{10}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario 1</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scenario 2</td>
<td style="text-align: right;">97</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">90</td>
<td style="text-align: right;">98</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scenario 3</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">89</td>
<td style="text-align: right;">88</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">34</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p><a href="#fig-all3" class="quarto-xref">Figure&nbsp;4</a> plots the distribution of the ARI differences between the model with either the selected variables or all the variables, and the one with the true clustering variables. These plots shows that for all scenarios, the ARI of the model with the selected variables (left boxplot of each plot) are always closest to the optimal ARI (obtained with the true clustering variables).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-all3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-all3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-all3-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-all3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Distribution of the ARI differences with the model with the true clustering variables, for the model with the selected variables and the model with all variables.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Finally <a href="#fig-all2" class="quarto-xref">Figure&nbsp;5</a> plots the histogram of the difference of ARI with the selected variables and with all the variables. This plot illustrates the interest of variable selection on the clustering results, and indeed, for all the scenarios, the ARI is better with the selected variables than when using all the variables.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-all2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-all2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-all2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-all2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Distribution of the ARI differences for the model with the selected variables and the model with all variables.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="international-ultrarunning-association-data" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> International Ultrarunning Association Data</h1>
<p>We apply the proposed procedure to the data from the 2012 International Ultrarunning Association World 24H Championships.</p>
<p>We start by initializing the stepwise algorithm, and find the variables selected by the screening procedure:</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fit_screen <span class="ot">&lt;-</span> <span class="fu">poissonmix_screen</span>(x, <span class="at">G =</span> <span class="dv">1</span><span class="sc">:</span>Gmax)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>jchosen <span class="ot">&lt;-</span> fit_screen<span class="sc">$</span>jchosen</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>jchosen</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> [1]  3  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24</code></pre>
</div>
</div>
<p>We then execute the proposed stepwise selection algorithm (the computing time is about 26 minutes on a laptop with 2.3 GHz Intel Core i7 processor and 32Go of RAM):</p>
<div class="cell">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">poissonmix_varsel</span>(x, <span class="at">jchosen =</span> jchosen, <span class="at">G =</span> <span class="dv">1</span><span class="sc">:</span>Gmax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The final chosen variables found by the algorithm are:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1]  9 10 11 12 14 15 16 17 18 19 20 21 22 24</code></pre>
</div>
</div>
<p>The optimal number of clusters 6 has been chosen inside the stepwise selection algorithm. The same choice is obtained when looking for the best <span class="math inline">G</span> with the conditionally independent Poisson mixture on the selected variables (<a href="#fig-BIC2" class="quarto-xref">Figure&nbsp;6</a>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-BIC2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-BIC2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-BIC2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-BIC2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Bayesian Information Criterion (BIC) for the independent Poisson mixture model with the seleceted variables.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In order to illustrate the results, we plot the cluster means according to the 24 variable mean parameters per cluster. For each variable not in the chosen variable set, a Poisson regression model is fitted with the chosen variables as predictors. Forward and backwards variable selection is conducted on this regression, if the regression model has any predictor variables, then the variable is called “redundant” and if the regression model has no predictor variables, then the variable is called “irrelevant”.</p>
<p><a href="#fig-Running-result1" class="quarto-xref">Figure&nbsp;7</a> shows the cluster mean for each variable, where the label indicates if the variable is irrelevant for clustering (“I”), redundant (“R”) or useful (the label is then the cluster number).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-Running-result1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Running-result1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-Running-result1-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Running-result1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Cluster means and usefulness of the variables.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The variables discriminate the clusters pacing strategies of the runners are the number of laps covered during the last two thirds of the race (except during the 13th and 23rd hours). The number of laps covered during the first eight hours does not provide any additional clustering information, and even no information at all for the number of laps covered during the first hour.</p>
<p><a href="#fig-Running-result2" class="quarto-xref">Figure&nbsp;8</a> shows boxplots of the total number of loops covered by the runners in each of the clusters.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-Running-result2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Running-result2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Jacques_Murphy_files/figure-html/fig-Running-result2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Running-result2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Number of loops covered by the runners of each clusters.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Cluster 5 are clearly the most efficient runners. Looking at the running strategy in <a href="#fig-Running-result1" class="quarto-xref">Figure&nbsp;7</a>, we can see that they start as runners of Cluster 1 and Cluster 2, but they managed to keep a constant pace on the second part of the race, unlike those of the other two clusters which faltered. Runners of Cluster 3 has covered the fewest number of laps. Indeed, looking at their running strategy, we can see that most of these runners stop after the first third of the race. Cluster 6 is relatively similar to Cluster 3, but runners manage to continue running until half of the race is completed. Finally, Cluster 4 obtains slightly better results than Cluster 6, starting more carefully, and managing to run until the end of the race, even if the pace of the last hours is not very constant.</p>
</section>
<section id="discussion" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Discussion</h1>
<p>A method for clustering and variable selection for multivariate count data has been proposed. The method is shown to give excellent performance on both simulated and real data examples. The method selects set of relevant variables for clustering and other variables are not selected if they are irrelevant or redundate for clustering purposes.</p>
<p>The proposed method is shown to give interesting insights in the application domain, where some clusters members are shown to perform better overall to others and the benefits of constant (or near constant pacing) are shown.</p>
<p>The level of variable selection is determined by the relative performance of the two models (as shown in <a href="#sec-interpretation" class="quarto-xref">Section&nbsp;4.2</a>) is compared. Alternative models to the Poisson GLM model which have greater flexibility could lead to a smaller set of selected variables than the proposed method achieves. This is a topic for future research.</p>
<p>The proposed method is based on a conditionally independent Poisson mixture model for the selected variables. It could be argued that the conditional independence assumption is unrealistic in the application. <span class="citation" data-cites="Hand2001">Hand and Yu (<a href="#ref-Hand2001" role="doc-biblioref">2001</a>)</span> consider the implication of incorrectly assuming conditional independence in a classification setting and show that it can make the group membership probabilities over confident. Furthermore, in the conditional independent Poisson mixture model, the number of clusters can be upwardly biased, where extra clusters are included to model dependence in the data. The approach taken in the paper could be extended to use other multivariate count distributions, including multivariate distributions without the conditional independence assumption <span class="citation" data-cites="Karlis2018 Karlis_2007 Inouye_2017">(eg. <a href="#ref-Karlis2018" role="doc-biblioref">Karlis 2018</a>; <a href="#ref-Karlis_2007" role="doc-biblioref">Karlis and Meligkotsidou 2007</a>; <a href="#ref-Inouye_2017" role="doc-biblioref">Inouye et al. 2017b</a>)</span>.</p>
<p>The code for the proposed approach will be made available as an R package.</p>
</section>
<section id="acknowlegements" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Acknowlegements</h1>
<p>This work was supported by the Science Foundation Ireland Insight Research Centre (SFI/12/RC/2289_P2) and a visit to the Collegium – Institut d’Études Avancées de Lyon.</p>
<!-- -->

</section>
<section id="bibliography" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti_2002" class="csl-entry" role="listitem">
Agresti, Alan. 2013. <em>Categorical Data Analysis</em>. Third. Wiley Series in Probability and Statistics. Wiley-Interscience [John Wiley &amp; Sons], Hoboken, NJ.
</div>
<div id="ref-Bouveyron_2019" class="csl-entry" role="listitem">
Bouveyron, Charles, Gilles Celeux, T. Brendan Murphy, and Adrian E. Raftery. 2019. <em>Model-Based Clustering and Classification for Data Science</em>. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge. <a href="https://doi.org/10.1017/9781108644181">https://doi.org/10.1017/9781108644181</a>.
</div>
<div id="ref-Chiquet2021" class="csl-entry" role="listitem">
Chiquet, Julien, Mahendra Mariadassou, and Stéphane Robin. 2021. <span>“The Poisson-Lognormal Model as a Versatile Framework for the Joint Analysis of Species Abundances.”</span> <em>Frontiers in Ecology and Evolution</em> 9: 188. <a href="https://doi.org/10.3389/fevo.2021.588292">https://doi.org/10.3389/fevo.2021.588292</a>.
</div>
<div id="ref-DeanRaftery2010" class="csl-entry" role="listitem">
Dean, Nema, and Adrian E. Raftery. 2010. <span>“Latent Class Analysis Variable Selection.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 62 (1): 11–35. <a href="https://doi.org/10.1007/s10463-009-0258-9">https://doi.org/10.1007/s10463-009-0258-9</a>.
</div>
<div id="ref-Dempster_1977" class="csl-entry" role="listitem">
Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. 1977. <span>“Maximum Likelihood from Incomplete Data via the <span>EM</span> Algorithm.”</span> <em>Journal of the Royal Statistical Society: Series B</em> 39: 1–38. <a href="https://doi.org/10.1111/j.2517-6161.1977.tb01600.x">https://doi.org/10.1111/j.2517-6161.1977.tb01600.x</a>.
</div>
<div id="ref-EverittHand1981" class="csl-entry" role="listitem">
Everitt, Brian S., and David J. Hand. 1981. <em>Finite Mixture Distributions</em>. Chapman &amp; Hall.
</div>
<div id="ref-Fop_2018" class="csl-entry" role="listitem">
Fop, Michael, and Thomas Brendan Murphy. 2018. <span>“Variable Selection Methods for Model-Based Clustering.”</span> <em>Statistics Surveys</em> 12: 18–65. <a href="https://doi.org/10.1214/18-SS119">https://doi.org/10.1214/18-SS119</a>.
</div>
<div id="ref-Fop_2017" class="csl-entry" role="listitem">
Fop, Michael, Keith Smart, and Thomas Brendan Murphy. 2017. <span>“Variable Selection for Latent Class Analysis with Application to Low Back Pain Diagnosis.”</span> <em>Annals of Applied Statistics.</em> 11: 2085–115.
</div>
<div id="ref-Fruhwirth_2018" class="csl-entry" role="listitem">
Frühwirth-Schnatter, Sylvia, Gilles Celeux, and Christian P. Robert. 2018. <em>Handbook of Mixture Analysis</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9780429055911">https://doi.org/10.1201/9780429055911</a>.
</div>
<div id="ref-Hand2001" class="csl-entry" role="listitem">
Hand, David J., and Keming Yu. 2001. <span>“Idiot’s Bayes—Not so Stupid After All?”</span> <em>International Statistical Review</em> 69 (3): 385–98. https://doi.org/<a href="https://doi.org/10.1111/j.1751-5823.2001.tb00465.x">https://doi.org/10.1111/j.1751-5823.2001.tb00465.x</a>.
</div>
<div id="ref-Hartigan_1979" class="csl-entry" role="listitem">
Hartigan, John A., and M. Anthony Wong. 1979. <span>“A k-Means Clustering Algorithm.”</span> <em>Applied Statistics</em> 28 (1): 100–108.
</div>
<div id="ref-HubertArabie1985" class="csl-entry" role="listitem">
Hubert, Lawrence, and Phipps Arable. 1985. <span>“Comparing Partitions.”</span> <em>Journal of Classification</em> 2 (1): 193–218. <a href="https://doi.org/10.1007/BF01908075">https://doi.org/10.1007/BF01908075</a>.
</div>
<div id="ref-Inouye1998" class="csl-entry" role="listitem">
Inouye, David I., Eunho Yang, Genevera I. Allen, and Pradeep Ravikumar. 2017a. <span>“A Review of Multivariate Distributions for Count Data Derived from the Poisson Distribution.”</span> <em>WIREs Computational Statistics</em> 9 (3): e1398. https://doi.org/<a href="https://doi.org/10.1002/wics.1398">https://doi.org/10.1002/wics.1398</a>.
</div>
<div id="ref-Inouye_2017" class="csl-entry" role="listitem">
———. 2017b. <span>“A Review of Multivariate Distributions for Count Data Derived from the Poisson Distribution.”</span> <em>WIREs Computational Statistics</em> 9 (3): e1398. <a href="https://doi.org/10.1002/wics.1398">https://doi.org/10.1002/wics.1398</a>.
</div>
<div id="ref-Karlis2018" class="csl-entry" role="listitem">
Karlis, Dimitris. 2018. <span>“Mixture Modelling of Discrete Data.”</span> In <em>Handbook of Mixture Analysis</em>, edited by Sylvia Frühwirth-Schnatter, Gilles Celeux, and Christian P. Robert, 193–218. CRC Press.
</div>
<div id="ref-Karlis_2007" class="csl-entry" role="listitem">
Karlis, Dimitris, and Loukia Meligkotsidou. 2007. <span>“Finite Mixtures of Multivariate Poisson Distributions with Application.”</span> <em>Journal of Statistical Planning and Inference</em> 137 (6): 1942–60. <a href="https://doi.org/10.1016/j.jspi.2006.07.001">https://doi.org/10.1016/j.jspi.2006.07.001</a>.
</div>
<div id="ref-Kass_1995" class="csl-entry" role="listitem">
Kass, Robert E., and Adrian E. Raftery. 1995. <span>“Bayes Factors.”</span> <em>Journal of the American Statististical Association</em> 90 (430): 773–95. <a href="https://doi.org/10.1080/01621459.1995.10476572">https://doi.org/10.1080/01621459.1995.10476572</a>.
</div>
<div id="ref-MaugisEtAl2009" class="csl-entry" role="listitem">
Maugis, Cathy, Gilles Celeux, and Marie-Laure Martin-Magniette. 2009. <span>“Variable Selection in Model-Based Clustering: A General Variable Role Modeling.”</span> <em>Computational Statistics &amp; Data Analysis</em> 53 (11): 3872–82. <a href="https://doi.org/10.1016/j.csda.2009.04.013">https://doi.org/10.1016/j.csda.2009.04.013</a>.
</div>
<div id="ref-McLachlanPeel2000" class="csl-entry" role="listitem">
McLachlan, Geoffrey, and David Peel. 2000. <em>Finite Mixture Models</em>. New York: Wiley. <a href="https://doi.org/10.1002/0471721182">https://doi.org/10.1002/0471721182</a>.
</div>
<div id="ref-McParlandMurphy2018" class="csl-entry" role="listitem">
McParland, Damien, and Thomas Brendan Murphy. 2018. <span>“Mixture Modelling of High-Dimensional Data.”</span> In <em>Handbook of Mixture Analysis</em>, edited by Sylvia Frühwirth-Schnatter, Gilles Celeux, and Christian P. Robert, 247–80. CRC Press.
</div>
<div id="ref-RafteryDean2006" class="csl-entry" role="listitem">
Raftery, Adrian E., and Nema Dean. 2006. <span>“Variable Selection for Model-Based Clustering.”</span> <em>Journal of the American Statistical Association</em> 101 (473): 168–78. <a href="https://doi.org/10.1198/016214506000000113">https://doi.org/10.1198/016214506000000113</a>.
</div>
<div id="ref-Rand_1971" class="csl-entry" role="listitem">
Rand, William M. 1971. <span>“Objective Criteria for the Evaluation of Clustering Methods.”</span> <em>Journal of the American Statististical Association</em> 66 (336): 846–50.
</div>
<div id="ref-Rau2015" class="csl-entry" role="listitem">
Rau, Andrea, Cathy Maugis-Rabusseau, Marie-Laure Martin-Magniette, and Gilles Celeux. 2015. <span>“<span class="nocase">Co-expression analysis of high-throughput transcriptome sequencing data with Poisson mixture models</span>.”</span> <em>Bioinformatics</em> 31 (9): 1420–27. <a href="https://doi.org/10.1093/bioinformatics/btu845">https://doi.org/10.1093/bioinformatics/btu845</a>.
</div>
<div id="ref-Schwarz_1978" class="csl-entry" role="listitem">
Schwarz, Gideon. 1978. <span>“<span class="nocase">Estimating the Dimension of a Model</span>.”</span> <em>The Annals of Statistics</em> 6 (2): 461–64. <a href="https://doi.org/10.1214/aos/1176344136">https://doi.org/10.1214/aos/1176344136</a>.
</div>
<div id="ref-Silva2019" class="csl-entry" role="listitem">
Silva, Anjali, Steven J. Rothstein, Paul D. McNicholas, and Sanjeena Subedi. 2019. <span>“A Multivariate Poisson-Log Normal Mixture Model for Clustering Transcriptome Sequencing Data.”</span> <em>BMC Bioinformatics</em> 20 (1): 394. <a href="https://doi.org/10.1186/s12859-019-2916-0">https://doi.org/10.1186/s12859-019-2916-0</a>.
</div>
<div id="ref-WhiteMurphy2016" class="csl-entry" role="listitem">
White, Arthur, and Thomas Brendan Murphy. 2016. <span>“Exponential Family Mixed Membership Models for Soft&nbsp;Clustering of Multivariate Data.”</span> <em>Advances in Data Analysis and Classification</em> 10: 521–40.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{jacques2025,
  author = {Jacques, Julien and Brendan Murphy, Thomas},
  publisher = {Société Française de Statistique},
  title = {Model-Based {Clustering} and {Variable} {Selection} for
    {Multivariate} {Count} {Data}},
  journal = {Computo},
  date = {2025-03-18},
  url = {https://computo.sfds.asso.fr/template-computo-quarto},
  doi = {xxxx},
  issn = {2824-7795},
  langid = {en},
  abstract = {Model-based clustering provides a principled way of
    developing clustering methods. We develop a new model-based
    clustering methods for count data. The method combines clustering
    and variable selection for improved clustering. The method is based
    on conditionally independent Poisson mixture models and Poisson
    generalized linear models. The method is demonstrated on simulated
    data and data from an ultra running race, where the method yields
    excellent clustering and variable selection performance.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-jacques2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Jacques, Julien, and Thomas Brendan Murphy. 2025. <span>“Model-Based
Clustering and Variable Selection for Multivariate Count Data.”</span>
<em>Computo</em>, March. <a href="https://doi.org/xxxx">https://doi.org/xxxx</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb9" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{r setup, include=FALSE}</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::opts_chunk$set(tidy = FALSE, fig.width = 5, fig.height = 5, echo = FALSE)</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r packages2, message = FALSE, warning=FALSE, eval=TRUE}</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="in">library("mclust")</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="in">library("rio")</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="in">library("e1071")</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="in">library("plotly")</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="in">library("seriation")</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="in">library("magick")</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="in">```{r functions, message = FALSE}</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="in">source("./functions/poissonmix.R")</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>Multivariate count data is ubiquitous in statistical applications, as ecology <span class="co">[</span><span class="ot">@Chiquet2021</span><span class="co">]</span>, genomics <span class="co">[</span><span class="ot">@Rau2015; @Silva2019</span><span class="co">]</span>. These data arise when each observation consists of a vector of count values.</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>Count data are often treated as continuous data and therefore modeled by a</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>Gaussian distribution, this assumption is particularly poor when the</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>measured counts are low. Instead, we use the reference</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>distribution for count data which is the Poisson distribution</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Agresti_2002; @Inouye1998</span><span class="co">]</span>.</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>When a data set is heterogeneous, clustering allows to extract</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>homogeneous subsets from the whole data set. Many clustering methods,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>such as $k$-means <span class="co">[</span><span class="ot">@Hartigan_1979</span><span class="co">]</span>, are geometric in nature, whereas many modern clustering approaches are based</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>on probabilistic models. In this work, we use model-based clustering which has been developed for many types of data <span class="co">[</span><span class="ot">@Bouveyron_2019; @McLachlanPeel2000; @Fruhwirth_2018</span><span class="co">]</span>.</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>Modern data are often high-dimensional, that is the number of variables is</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>often large. Among these variables, some are useful for the task of</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>interest, some are useless for the task of interest and some others are useful but redundant.</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>There is a need to select only the relevant variables, and that</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>whatever is the task. Variable selection methods are widespread for supervised learning tasks, in particular to avoid overfitting. However, variable selection methods are less well developed for unsupervised learning tasks, such as clustering. Recently, several</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>methods have been proposed for selecting the relevant variables in</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>model-based clustering; we refer to @Fop_2018 and @McParlandMurphy2018 for recent detailed surveys.</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>The goal of the present work is to provide a clustering and variable selection method</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>for multivariate count data, which, to the best of our knowledge, has</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>not yet been studied in depth. A methodology based on a conditionally independent Poisson mixture is developed to achieve this goal. The method yields a final clustering model which is a conditionally independent Poisson mixture model for a subset of the variables.</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="fu"># Motivating Example</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>The International Association of Ultrarunners (IAU) 24 hour World</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>Championships were held in Katowice, Poland from September 8th to 9th,</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="ss">2012. </span>Two hundred and sixty athletes representing twenty four countries</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>entered the race, which was held on a course consisting of a 1.554 km</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>looped route. An update of the number of laps covered by each athlete</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>was recorded approximately every hour <span class="co">[</span><span class="ot">@WhiteMurphy2016</span><span class="co">]</span>. @fig-24H plots the number of</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>loops recorded each hour for the three medalists.</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="in">```{r medalists}</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-24H</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Number of loops per hour for the three medalists."</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="in">load("X24H.Rdata")</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a><span class="in">tmp &lt;- sort(as.vector(rowSums(x)), decreasing = TRUE, index.return = TRUE)</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="in">matplot(t(matrix(1:24, 3, 24,byrow = TRUE)), t(x[tmp$ix[1:3], 1:24]),type = "o",lty = 3,xlab = 'Hours', ylab = "Number of loops", xaxt = "n", yaxt = "n", pch = 1:3)</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="in">axis(1, at = seq(6, 24, 6),labels = seq(6, 24, 6))</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a><span class="in">axis(2, at = 6:9, labels = 6:9)</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a><span class="in">legend("topright", 1:3, col = 1:3, legend=c("Gold medal", "Silver medal", "Bronze medal"), pch = 1:3, cex = 0.8)</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>We can see among these three runners different strategies, the second placed</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>runner lapped at a regular rate, the first placed runner had a fast start but slowed later, and the third</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>placed runner also started fast but slowed more than the first place runner. </span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>Our first goal will be, to analyze the whole data set to identify the different running strategies and to</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>evaluate which strategies are the best ones. The second goal is to identify which</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>variables allows to distinguish between the clusters, in order to identify which</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>hour is essential in the management of this endurance race.</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a><span class="fu"># Independent Poisson Mixture</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>Let $X_n=(X_{n1}, X_{n2}, \ldots, X_{nM})$ be a random vector of counts</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>for $n=1, 2, \ldots, N$. The goal is to clusters theses $N$ observations</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>into $G$ clusters. Let $Z_n=(Z_{n1}, Z_{n2}, \ldots, Z_{nG})$ be the</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>latent cluster indicator vector, where $Z_{ng}=1$ if observation $n$</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>belongs to cluster $g$ and $Z_{ng}=0$ otherwise. We assume that</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>${\mathbb P}<span class="sc">\{</span>Z_{ng}=1<span class="sc">\}</span> = \tau_g$ for $g=1,2, \ldots, G$. Let denote</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>$\tau=(\tau_1,\ldots,\tau_G)$. The conditionally independent Poisson mixture model <span class="co">[</span><span class="ot">@Karlis2018, Section 9.4.2.1</span><span class="co">]</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>assumes that the elements of $X_n$ are independent Poisson distributed</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>random variables, conditional on $Z_n$. That is, </span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>Z_n &amp; \sim  {\rm Multinomial}(1, \tau)<span class="sc">\\</span></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>X_{nm}|(Z_{ng}=1) &amp; \sim  {\rm Poisson}(\lambda_{gm}), {\rm ~for~} m=1, 2, \ldots, M.</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>Alternative modelling frameworks exist, either to introduce</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>some dependence between variables or to normalize the variables. We</span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>refer the interested reader to <span class="co">[</span><span class="ot">@Karlis2018; @Bouveyron_2019, Chapter 6</span><span class="co">]</span> for more details.</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>Denoting the model parameters by $\theta=(\tau,\lambda)$ where</span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>$\lambda=(\lambda_{gm})_{1\leq g \leq G, 1\leq m \leq M}$, and where</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>$X=(x_n)_{1\leq n \leq N}$ denotes the observations, the observed</span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>likelihood is </span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a>L(\theta)=\sum_{n=1}^{N}\sum_{g=1}^{G}\tau_g\prod_{m=1}^M\phi(x_{nm},\lambda_{gm}),</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a>where $\phi(x,\lambda)=\exp(-\lambda)\lambda^{x}/x!$, the Poisson probability mass function.</span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>Due to form of the mixture distribution, there are no closed form for the maximum</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>likelihood estimators, and an iterative EM algorithm needs to be used</span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Dempster_1977</span><span class="co">]</span> to maximize the likelihood. The EM algorithm consists, starts from an initial</span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>value $\theta^{(0)}$ for the model parameter, and alternates the two</span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a>following steps until convergence of the likelihood.</span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>At the $q$th iteration of the EM algorithm, the E-step consists of</span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>computing for all $1\leq n\leq N$ and $1\leq g\leq G$:</span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>$$t_{ng}^{(q)}=\frac{\tau_g^{(q)}\prod_{m=1}^M\phi(x_{nm},\lambda_{gm})}{\sum_{h=1}^{G}\tau_h^{(q)}\prod_{m=1}^M\phi(x_{nm},\lambda_{hm})}.$$</span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a>In the M-step, the model parameters are updated as follows:</span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a>$$ \tau_g^{(q+1)}=\frac{\sum_{n=1}^Nt_{ng}^{(q)}}{N}</span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a>\quad {\rm ~and~} \quad</span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>\lambda_{gm}^{(q+1)}=\frac{\sum_{n=1}^Nt_{ng}^{(q)}x_{nm}}{\sum_{n=1}^Nt_{ng}^{(q)}}.</span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a>The EM algorithm steps are iterated until convergence, where convergence is determined when $\log L(\theta^{(q+1)}) - \log L(\theta^{(q)}) &lt; \epsilon$.</span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a>The number of clusters $G$ is selected using the Bayesian information criterion (BIC)</span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Schwarz_1978</span><span class="co">]</span>,</span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a>BIC= 2 \log L(\hat\theta) - <span class="sc">\{</span>(G - 1) + GM<span class="sc">\}</span>\log(N),</span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a>where $\hat\theta$ is the maximum likelihood estimate of the model parameters; models with higher BIC are prefered to models with lower BIC.</span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a><span class="fu"># Variable selection</span></span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>We develop a model-based clustering method with variable selection for</span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a>multivariate count data. The method follows the approach of [@RafteryDean2006;</span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a>@MaugisEtAl2009] for continuous data and <span class="co">[</span><span class="ot">@DeanRaftery2010; @Fop_2017</span><span class="co">]</span></span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a>for categorical data. It consists in a stepwise model comparison</span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a>approach where variables are added and removed from a set of clustering</span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>variables. </span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model setup</span></span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a>The clustering and variable selection approach is based around partitioning</span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>${X}_n=(X_n^{C}, X_n^{P}, X_n^{O})$ into three parts:</span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$X_n^{C}$: The current clustering variables,</span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$X_n^{P}$: The proposed variable to add to the clustering variables,</span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$X_n^{O}$: The other variables.</span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>For simplicity of notation, $C$ will be used to denote the set of indices of the current clustering variables, $P$ the indices of the proposed variable and $O$ the indices of the other one. Then $(C,P,O)$ is a partition of $<span class="sc">\{</span>1,\ldots,M<span class="sc">\}</span>$.</span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>The decision on whether to add the proposed variable to the clustering</span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>variables is based on comparing two models:</span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>${\cal M}_1$ (Clustering Model), which assumes that the proposed</span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>variable is useful for clustering: </span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a>(X_n^C, X_n^P) \sim \sum_{g=1}^{G} \tau_g \prod_{m\in<span class="sc">\{</span>C, P<span class="sc">\}</span>} {\rm Poisson}(\lambda_{gm}).</span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>The ${\cal M}_1$ model is fitted for different values of $G$ between $1$ and $G_{max}$ to achieve the best clustering model.</span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>${\cal M}_2$ (Non-Clustering Model) which assumes that the proposed</span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>variable is not useful for clustering, but is potentially linked to the clustering</span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>variables through a Poisson GLM, that is, </span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a>X_n^C &amp;\sim \sum_{g=1}^{G}\tau_g \prod_{m\in C} {\rm Poisson}(\lambda_{gm})<span class="sc">\\</span></span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a>X_n^P| (X_n^C=x_n^C, Z_{ng}=1) &amp; \sim  {\rm PoissonGLM}(x_n^{(C)}),</span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a>where Poisson GLM states that</span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a>\log {\mathbb E}<span class="co">[</span><span class="ot">X_n^P|X_n^C=x_n^C, Z_{ng}=1</span><span class="co">]</span> = \alpha + \beta^\top x_n^C.</span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a>In order to avoid non significant terms in the Poisson GLM</span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a>model, a standard stepwise variable selection approach (using BIC as the variable selection criterion) is considered.</span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a>Thus, the proposed variable $X_n^P$ will be dependent on</span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a>only a subset $X_n^R$ of the clustering variables $X_n^C$.</span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a>We note that $G$ is fixed in the non-clustering model, because an optimal value for $G$ is previously chosen. </span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a>The clustering and non-clustering models are represented as graphical</span>
<span id="cb9-182"><a href="#cb9-182" aria-hidden="true" tabindex="-1"></a>models in @fig-graphical.</span>
<span id="cb9-183"><a href="#cb9-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a>:::{#fig-graphical layout="[<span class="co">[</span><span class="ot">-10,80,10</span><span class="co">]</span>]" }</span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a><span class="in">```{.tikz}</span></span>
<span id="cb9-187"><a href="#cb9-187" aria-hidden="true" tabindex="-1"></a><span class="in">%%| filename: ../figures/fig-graphical</span></span>
<span id="cb9-188"><a href="#cb9-188" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{tikzpicture}</span></span>
<span id="cb9-189"><a href="#cb9-189" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[draw, circle] at (0, 0) (z) {$Z$};</span></span>
<span id="cb9-190"><a href="#cb9-190" aria-hidden="true" tabindex="-1"></a><span class="in">  \node[draw] at (-2, -2) (xc) {$X^C$};</span></span>
<span id="cb9-191"><a href="#cb9-191" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[draw] at (2, -2) (xp) {$X^P$};</span></span>
<span id="cb9-192"><a href="#cb9-192" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[draw] at (0, -4) (xo) {$X^O$};</span></span>
<span id="cb9-193"><a href="#cb9-193" aria-hidden="true" tabindex="-1"></a><span class="in"> \draw[-&gt;] (z) -- (xp);</span></span>
<span id="cb9-194"><a href="#cb9-194" aria-hidden="true" tabindex="-1"></a><span class="in"> \draw[-&gt;] (z) -- (xc);</span></span>
<span id="cb9-195"><a href="#cb9-195" aria-hidden="true" tabindex="-1"></a><span class="in">\draw[-&gt;] (xc) -- (xo);</span></span>
<span id="cb9-196"><a href="#cb9-196" aria-hidden="true" tabindex="-1"></a><span class="in">\draw[-&gt;] (xp) -- (xo);</span></span>
<span id="cb9-197"><a href="#cb9-197" aria-hidden="true" tabindex="-1"></a><span class="in">\node[draw, rectangle] at (0, 0.75) {Clustering};</span></span>
<span id="cb9-198"><a href="#cb9-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-199"><a href="#cb9-199" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[draw, circle] at (7, 0) (z) {$Z$};</span></span>
<span id="cb9-200"><a href="#cb9-200" aria-hidden="true" tabindex="-1"></a><span class="in">  \node[draw] at (5, -2) (xc) {$X^C$};</span></span>
<span id="cb9-201"><a href="#cb9-201" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[draw] at (9, -2) (xp) {$X^P$};</span></span>
<span id="cb9-202"><a href="#cb9-202" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[draw] at (7, -4) (xo) {$X^O$};</span></span>
<span id="cb9-203"><a href="#cb9-203" aria-hidden="true" tabindex="-1"></a><span class="in"> \draw[-&gt;] (z) -- (xc);</span></span>
<span id="cb9-204"><a href="#cb9-204" aria-hidden="true" tabindex="-1"></a><span class="in"> \draw[dashed,-&gt;] (xc) -- (xp);</span></span>
<span id="cb9-205"><a href="#cb9-205" aria-hidden="true" tabindex="-1"></a><span class="in"> \node[fill=white] at (7, -1.7) {$X^R\subseteq X^C$};</span></span>
<span id="cb9-206"><a href="#cb9-206" aria-hidden="true" tabindex="-1"></a><span class="in">\draw[-&gt;] (xc) -- (xo);</span></span>
<span id="cb9-207"><a href="#cb9-207" aria-hidden="true" tabindex="-1"></a><span class="in">\draw[-&gt;] (xp) -- (xo);</span></span>
<span id="cb9-208"><a href="#cb9-208" aria-hidden="true" tabindex="-1"></a><span class="in">\node[draw, rectangle] at (7, 0.75) {Non Clustering};</span></span>
<span id="cb9-209"><a href="#cb9-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-210"><a href="#cb9-210" aria-hidden="true" tabindex="-1"></a><span class="in">\end{tikzpicture}</span></span>
<span id="cb9-211"><a href="#cb9-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-212"><a href="#cb9-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-213"><a href="#cb9-213" aria-hidden="true" tabindex="-1"></a>Graphical model representations of the clustering and non-clustering models.</span>
<span id="cb9-214"><a href="#cb9-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-215"><a href="#cb9-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-216"><a href="#cb9-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-217"><a href="#cb9-217" aria-hidden="true" tabindex="-1"></a>Thus, there is two reasons for which ${\cal M}_2$ can be preferred to</span>
<span id="cb9-218"><a href="#cb9-218" aria-hidden="true" tabindex="-1"></a>${\cal M}_1$: either $X_n^P$ does not contain information about the</span>
<span id="cb9-219"><a href="#cb9-219" aria-hidden="true" tabindex="-1"></a>latent clustering variable at all (ie. $X_n^R=\emptyset$), or $X_n^P$ does not add</span>
<span id="cb9-220"><a href="#cb9-220" aria-hidden="true" tabindex="-1"></a>further useful information about the clustering given the information</span>
<span id="cb9-221"><a href="#cb9-221" aria-hidden="true" tabindex="-1"></a>already contained in the current clustering variables. In the first situation, we say that $X_n^P$ is an irrelevant variable, because it contains no clustering information. In the second</span>
<span id="cb9-222"><a href="#cb9-222" aria-hidden="true" tabindex="-1"></a>situation, we say that $X_n^P$ is a redundant variable because it contains no extra information about the clustering beyond the current clustering variables ($X_n^C$). </span>
<span id="cb9-223"><a href="#cb9-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-224"><a href="#cb9-224" aria-hidden="true" tabindex="-1"></a>Additionally, both models assume the same form for the conditional</span>
<span id="cb9-225"><a href="#cb9-225" aria-hidden="true" tabindex="-1"></a>distribution for $X_n^{O}|(X_{n}^{C}, X_{n}^{P})$ and whose form doesn't</span>
<span id="cb9-226"><a href="#cb9-226" aria-hidden="true" tabindex="-1"></a>need to be explicitly specified because it doesn't affect the model</span>
<span id="cb9-227"><a href="#cb9-227" aria-hidden="true" tabindex="-1"></a>choice.</span>
<span id="cb9-228"><a href="#cb9-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-229"><a href="#cb9-229" aria-hidden="true" tabindex="-1"></a>Variable $P$ is added to $C$ if the clustering model (${\cal M}_1$) is</span>
<span id="cb9-230"><a href="#cb9-230" aria-hidden="true" tabindex="-1"></a>preferred to the non-clustering model (${\cal M}_2$). In order to</span>
<span id="cb9-231"><a href="#cb9-231" aria-hidden="true" tabindex="-1"></a>compare ${\cal M}_1$ and ${\cal M}_2$, following <span class="co">[</span><span class="ot">@DeanRaftery2010</span><span class="co">]</span>, we</span>
<span id="cb9-232"><a href="#cb9-232" aria-hidden="true" tabindex="-1"></a>consider the Bayes Factor:</span>
<span id="cb9-233"><a href="#cb9-233" aria-hidden="true" tabindex="-1"></a>$$B_{1,2}=\frac{p(X|{\cal M}_1)}{p(X|{\cal M}_2)}$$ which is</span>
<span id="cb9-234"><a href="#cb9-234" aria-hidden="true" tabindex="-1"></a>asymptotically approximated <span class="co">[</span><span class="ot">@Fop_2017; @Kass_1995</span><span class="co">]</span> using the </span>
<span id="cb9-235"><a href="#cb9-235" aria-hidden="true" tabindex="-1"></a>difference of the BIC criteria for both models:</span>
<span id="cb9-236"><a href="#cb9-236" aria-hidden="true" tabindex="-1"></a>$$2\log B_{1,2}\simeq BIC_{{\cal M}_1}-BIC_{{\cal M}_2}.$$</span>
<span id="cb9-237"><a href="#cb9-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-238"><a href="#cb9-238" aria-hidden="true" tabindex="-1"></a>The same modelling framework can be used for removing variables from the</span>
<span id="cb9-239"><a href="#cb9-239" aria-hidden="true" tabindex="-1"></a>current set of clustering variables.</span>
<span id="cb9-240"><a href="#cb9-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-241"><a href="#cb9-241" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpretation {#sec-interpretation}</span></span>
<span id="cb9-242"><a href="#cb9-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-243"><a href="#cb9-243" aria-hidden="true" tabindex="-1"></a>Comparing ${\cal M}_1$ and ${\cal M}_2$ is equivalent to comparing the</span>
<span id="cb9-244"><a href="#cb9-244" aria-hidden="true" tabindex="-1"></a>following $X_n^P|(X_n^C=x_n^C)$ structures.</span>
<span id="cb9-245"><a href="#cb9-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-246"><a href="#cb9-246" aria-hidden="true" tabindex="-1"></a>The ${\cal M}_1$ (Clustering Model) assumes that,</span>
<span id="cb9-247"><a href="#cb9-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-248"><a href="#cb9-248" aria-hidden="true" tabindex="-1"></a>X_n^P| (X_n^C=x_n^C) \sim \sum_{g=1}^{G} {\mathbb P}<span class="sc">\{</span>Z_{ng}=1|X_{n}^{C}=x_{n}^{C}<span class="sc">\}</span> {\rm Poisson}(\lambda_{gm}),</span>
<span id="cb9-249"><a href="#cb9-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-250"><a href="#cb9-250" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb9-251"><a href="#cb9-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-252"><a href="#cb9-252" aria-hidden="true" tabindex="-1"></a>{\mathbb P}<span class="sc">\{</span>Z_{ng}=1|X_{n}^{C}=x_{n}^{C}<span class="sc">\}</span> = \frac{\tau_g \prod_{m=1}^{M}\phi(x_{nm}, \lambda_{gm})}{\sum_{h=1}^{G}\tau_h \prod_{m=1}^{M}\phi(x_{nm}, \lambda_{hm})}.</span>
<span id="cb9-253"><a href="#cb9-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-254"><a href="#cb9-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-255"><a href="#cb9-255" aria-hidden="true" tabindex="-1"></a>Whereas, the ${\cal M}_2$ (Non-Clustering Model) assumes that,</span>
<span id="cb9-256"><a href="#cb9-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-257"><a href="#cb9-257" aria-hidden="true" tabindex="-1"></a>X_n^P| (X_n^C=x_n^C) =  {\rm PoissonGLM}(x_n^C).</span>
<span id="cb9-258"><a href="#cb9-258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-259"><a href="#cb9-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-260"><a href="#cb9-260" aria-hidden="true" tabindex="-1"></a>The method contrasts which of conditional model structures is better</span>
<span id="cb9-261"><a href="#cb9-261" aria-hidden="true" tabindex="-1"></a>describing the distribution of the proposed variable $X^P$. The</span>
<span id="cb9-262"><a href="#cb9-262" aria-hidden="true" tabindex="-1"></a>clustering model (${\cal M}_1$) uses a mixture model, with covariate</span>
<span id="cb9-263"><a href="#cb9-263" aria-hidden="true" tabindex="-1"></a>dependent weights, for the conditional model whereas the non-clustering</span>
<span id="cb9-264"><a href="#cb9-264" aria-hidden="true" tabindex="-1"></a>model (${\cal M}_2$) is a Poisson generalized linear model. The model</span>
<span id="cb9-265"><a href="#cb9-265" aria-hidden="true" tabindex="-1"></a>selection criterion chooses the model that best models this conditional</span>
<span id="cb9-266"><a href="#cb9-266" aria-hidden="true" tabindex="-1"></a>distribution.</span>
<span id="cb9-267"><a href="#cb9-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-268"><a href="#cb9-268" aria-hidden="true" tabindex="-1"></a><span class="fu">## Stepwise selection algorithm</span></span>
<span id="cb9-269"><a href="#cb9-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-270"><a href="#cb9-270" aria-hidden="true" tabindex="-1"></a><span class="fu">### Screening variables: Initialization</span></span>
<span id="cb9-271"><a href="#cb9-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-272"><a href="#cb9-272" aria-hidden="true" tabindex="-1"></a>We start with an initial choice of $C$ by first screening each</span>
<span id="cb9-273"><a href="#cb9-273" aria-hidden="true" tabindex="-1"></a>individual variable by fitting a mixture of univariate Poisson distributions <span class="co">[</span><span class="ot">eg. @EverittHand1981, Chapter 4.3</span><span class="co">]</span>,</span>
<span id="cb9-274"><a href="#cb9-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-275"><a href="#cb9-275" aria-hidden="true" tabindex="-1"></a>X_{nm} \sim \sum_{g=1}^{G}\tau_g {\rm Poisson}(\lambda_{gm}), {\rm ~for~} G=1,2,\ldots, G_{max}.</span>
<span id="cb9-276"><a href="#cb9-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-277"><a href="#cb9-277" aria-hidden="true" tabindex="-1"></a>The initial set of variables is set to be those variables where any</span>
<span id="cb9-278"><a href="#cb9-278" aria-hidden="true" tabindex="-1"></a>model with $G&gt;1$ is preferred to the $G=1$ model.</span>
<span id="cb9-279"><a href="#cb9-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-280"><a href="#cb9-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stepwise algorithm: Updating</span></span>
<span id="cb9-281"><a href="#cb9-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-282"><a href="#cb9-282" aria-hidden="true" tabindex="-1"></a>We consider a stepwise algorithm which alternates between</span>
<span id="cb9-283"><a href="#cb9-283" aria-hidden="true" tabindex="-1"></a>adding and removing steps. In the removal step, all the variables in</span>
<span id="cb9-284"><a href="#cb9-284" aria-hidden="true" tabindex="-1"></a>$X^C$ are examined in turn to be removed from the set. In the adding</span>
<span id="cb9-285"><a href="#cb9-285" aria-hidden="true" tabindex="-1"></a>step, all the variables in $X^O$ are examined in turn to be added to the</span>
<span id="cb9-286"><a href="#cb9-286" aria-hidden="true" tabindex="-1"></a>clustering set.</span>
<span id="cb9-287"><a href="#cb9-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-288"><a href="#cb9-288" aria-hidden="true" tabindex="-1"></a>The algorithm also performs the selection of the number $G$ of clusters</span>
<span id="cb9-289"><a href="#cb9-289" aria-hidden="true" tabindex="-1"></a>finding at each stage the optimal combination of clustering variables</span>
<span id="cb9-290"><a href="#cb9-290" aria-hidden="true" tabindex="-1"></a>and number of clusters. The procedure stops when no change has been made</span>
<span id="cb9-291"><a href="#cb9-291" aria-hidden="true" tabindex="-1"></a>to the set $X^C$ after consecutive exclusion and inclusion steps.</span>
<span id="cb9-292"><a href="#cb9-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-293"><a href="#cb9-293" aria-hidden="true" tabindex="-1"></a>With the present stepwise selection algorithm, it can occur that</span>
<span id="cb9-294"><a href="#cb9-294" aria-hidden="true" tabindex="-1"></a>during the process, we get back on a solution (a set of clustering</span>
<span id="cb9-295"><a href="#cb9-295" aria-hidden="true" tabindex="-1"></a>variable) already explored. Since our algorithm is not stochastic, we</span>
<span id="cb9-296"><a href="#cb9-296" aria-hidden="true" tabindex="-1"></a>fall into an infinite cycle. In this situation the algorithm is stopped,</span>
<span id="cb9-297"><a href="#cb9-297" aria-hidden="true" tabindex="-1"></a>and the best solution according to BIC among the solution of the cycle</span>
<span id="cb9-298"><a href="#cb9-298" aria-hidden="true" tabindex="-1"></a>is kept.</span>
<span id="cb9-299"><a href="#cb9-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-300"><a href="#cb9-300" aria-hidden="true" tabindex="-1"></a><span class="fu"># Simulation study</span></span>
<span id="cb9-301"><a href="#cb9-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-302"><a href="#cb9-302" aria-hidden="true" tabindex="-1"></a>In this section, we evaluate the proposed variable selection method</span>
<span id="cb9-303"><a href="#cb9-303" aria-hidden="true" tabindex="-1"></a>through three different simulation scenarios. We start with an</span>
<span id="cb9-304"><a href="#cb9-304" aria-hidden="true" tabindex="-1"></a>illustrative example in which, using a data set simulated according to</span>
<span id="cb9-305"><a href="#cb9-305" aria-hidden="true" tabindex="-1"></a>the proposed model, we show how to perform the variable selection. </span>
<span id="cb9-306"><a href="#cb9-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-307"><a href="#cb9-307" aria-hidden="true" tabindex="-1"></a>Then, simulation studies are performed to evaluate the behavior of the</span>
<span id="cb9-308"><a href="#cb9-308" aria-hidden="true" tabindex="-1"></a>proposed selection method, when the data are simulated according to the</span>
<span id="cb9-309"><a href="#cb9-309" aria-hidden="true" tabindex="-1"></a>proposed model (Scenario1) and when the model</span>
<span id="cb9-310"><a href="#cb9-310" aria-hidden="true" tabindex="-1"></a>assumptions are violated. In Scenario2, the link</span>
<span id="cb9-311"><a href="#cb9-311" aria-hidden="true" tabindex="-1"></a>between $X^R$ and $X^C$ is no longer a Poisson GLM but a linear model. In</span>
<span id="cb9-312"><a href="#cb9-312" aria-hidden="true" tabindex="-1"></a>Scenario3, the clustering variables are no longer</span>
<span id="cb9-313"><a href="#cb9-313" aria-hidden="true" tabindex="-1"></a>conditionally independent.</span>
<span id="cb9-314"><a href="#cb9-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-315"><a href="#cb9-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## Illustrative example</span></span>
<span id="cb9-316"><a href="#cb9-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-317"><a href="#cb9-317" aria-hidden="true" tabindex="-1"></a><span class="in">```{r simulation1}</span></span>
<span id="cb9-318"><a href="#cb9-318" aria-hidden="true" tabindex="-1"></a><span class="in"># Generate data</span></span>
<span id="cb9-319"><a href="#cb9-319" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(69007)</span></span>
<span id="cb9-320"><a href="#cb9-320" aria-hidden="true" tabindex="-1"></a><span class="in">N &lt;- 400</span></span>
<span id="cb9-321"><a href="#cb9-321" aria-hidden="true" tabindex="-1"></a><span class="in">G &lt;- 3</span></span>
<span id="cb9-322"><a href="#cb9-322" aria-hidden="true" tabindex="-1"></a><span class="in">P &lt;- 10</span></span>
<span id="cb9-323"><a href="#cb9-323" aria-hidden="true" tabindex="-1"></a><span class="in">tau0 &lt;- c(0.4, 0.3, 0.3)</span></span>
<span id="cb9-324"><a href="#cb9-324" aria-hidden="true" tabindex="-1"></a><span class="in">lambda0 &lt;- matrix(c(1, 1, 1, 1, 2, 2, 1, 4, 4, 4, 4, 4), G, 4, byrow = TRUE)</span></span>
<span id="cb9-325"><a href="#cb9-325" aria-hidden="true" tabindex="-1"></a><span class="in">l0 &lt;- sample (1:G, prob = tau0, replace = TRUE, size = N)</span></span>
<span id="cb9-326"><a href="#cb9-326" aria-hidden="true" tabindex="-1"></a><span class="in">Ng &lt;- table(l0)</span></span>
<span id="cb9-327"><a href="#cb9-327" aria-hidden="true" tabindex="-1"></a><span class="in">Z0 &lt;- unmap(l0)</span></span>
<span id="cb9-328"><a href="#cb9-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-329"><a href="#cb9-329" aria-hidden="true" tabindex="-1"></a><span class="in">x &lt;- matrix(NA, N, P)</span></span>
<span id="cb9-330"><a href="#cb9-330" aria-hidden="true" tabindex="-1"></a><span class="in">for (g in 1:G)</span></span>
<span id="cb9-331"><a href="#cb9-331" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb9-332"><a href="#cb9-332" aria-hidden="true" tabindex="-1"></a><span class="in">  x[l0==g, 1:4] &lt;- matrix(rpois(4 * Ng[g], lambda0[g, ]), Ng[g], 4, byrow = TRUE)</span></span>
<span id="cb9-333"><a href="#cb9-333" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-334"><a href="#cb9-334" aria-hidden="true" tabindex="-1"></a><span class="in">#x[, 1:4] &lt;- x[, 1:4] + rpois(N, 1) </span></span>
<span id="cb9-335"><a href="#cb9-335" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,5] &lt;- rpois(N, exp(0.2 * x[, 2]))</span></span>
<span id="cb9-336"><a href="#cb9-336" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,6] &lt;- rpois(N, exp(x[, 1] * 0.2 - 0.1 * x[, 2]))</span></span>
<span id="cb9-337"><a href="#cb9-337" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,7] &lt;- rpois(N, exp(0.1 * (x[, 1] + x[, 3] + x[, 4])))</span></span>
<span id="cb9-338"><a href="#cb9-338" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,8] &lt;- rpois(N, 4)</span></span>
<span id="cb9-339"><a href="#cb9-339" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,9] &lt;- rpois(N, 2)</span></span>
<span id="cb9-340"><a href="#cb9-340" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,10] &lt;- rpois(N, 1)</span></span>
<span id="cb9-341"><a href="#cb9-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-342"><a href="#cb9-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-343"><a href="#cb9-343" aria-hidden="true" tabindex="-1"></a>In the first simulation setting we consider 10 Poisson random variables.</span>
<span id="cb9-344"><a href="#cb9-344" aria-hidden="true" tabindex="-1"></a>Variables $X_1$, $X_2$, $X_3$ and $X_4$ are the clustering variables,</span>
<span id="cb9-345"><a href="#cb9-345" aria-hidden="true" tabindex="-1"></a>distributed according to a mixture of $G=$ <span class="in">`r G`</span> independent Poisson</span>
<span id="cb9-346"><a href="#cb9-346" aria-hidden="true" tabindex="-1"></a>mixture distributions with mixing proportions <span class="in">`r tau0`</span>. Variables $X_5$,</span>
<span id="cb9-347"><a href="#cb9-347" aria-hidden="true" tabindex="-1"></a>$X_6$ and $X_7$ are redundant variables, each one generated dependent on</span>
<span id="cb9-348"><a href="#cb9-348" aria-hidden="true" tabindex="-1"></a>the clustering variables. These three variables are linked to the four</span>
<span id="cb9-349"><a href="#cb9-349" aria-hidden="true" tabindex="-1"></a>first ones through a Poisson GLM. The last three variables, $X_8$, $X_9$</span>
<span id="cb9-350"><a href="#cb9-350" aria-hidden="true" tabindex="-1"></a>and $X_{10}$ are irrelevant variables not related to the previous ones.</span>
<span id="cb9-351"><a href="#cb9-351" aria-hidden="true" tabindex="-1"></a>@tbl-ParamSimul shows the parameter of the Poisson distribution for each variable and each cluster.</span>
<span id="cb9-352"><a href="#cb9-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-353"><a href="#cb9-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-354"><a href="#cb9-354" aria-hidden="true" tabindex="-1"></a><span class="in">```{r tabParamSimul}</span></span>
<span id="cb9-355"><a href="#cb9-355" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-ParamSimul</span></span>
<span id="cb9-356"><a href="#cb9-356" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-357"><a href="#cb9-357" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-358"><a href="#cb9-358" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-359"><a href="#cb9-359" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: "True values of component parameters (Scenario 1)"</span></span>
<span id="cb9-360"><a href="#cb9-360" aria-hidden="true" tabindex="-1"></a><span class="in">lambda &lt;- matrix(c(1, 1, 1, 1, 2, 2, 1, 4, 4, 4, 4, 4), G, 4, byrow = TRUE)</span></span>
<span id="cb9-361"><a href="#cb9-361" aria-hidden="true" tabindex="-1"></a><span class="in">lambda=cbind(lambda,c('$\\lambda_{g5}$','$\\lambda_{g5}$','$\\lambda_{g5}$'),c('$\\lambda_{g6}$','$\\lambda_{g6}$','$\\lambda_{g6}$'),c('$\\lambda_{g7}$','$\\lambda_{g7}$','$\\lambda_{g7}$'),c(4,4,4),c(2,2,2),c(1,1,1))</span></span>
<span id="cb9-362"><a href="#cb9-362" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(lambda) &lt;- c(paste("$\\lambda_{g", 1:P, "}$", sep =""))</span></span>
<span id="cb9-363"><a href="#cb9-363" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(lambda) &lt;- paste("$g=", 1:3, "$", sep="")</span></span>
<span id="cb9-364"><a href="#cb9-364" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::kable(lambda)</span></span>
<span id="cb9-365"><a href="#cb9-365" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-366"><a href="#cb9-366" aria-hidden="true" tabindex="-1"></a>with $\lambda_{g5}=\exp(0.2X_2)$, $\lambda_{g6}=\exp(0.2X_1-0.1X_2)$ and $\lambda_{g7}=\exp(0.1 (X_1+X_3+X_4))$.</span>
<span id="cb9-367"><a href="#cb9-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-368"><a href="#cb9-368" aria-hidden="true" tabindex="-1"></a>Below is the result obtained for one data set of size $N=$ <span class="in">`r N`</span>. The</span>
<span id="cb9-369"><a href="#cb9-369" aria-hidden="true" tabindex="-1"></a>evaluation criteria is the selected features (true one are $X_1$ to</span>
<span id="cb9-370"><a href="#cb9-370" aria-hidden="true" tabindex="-1"></a>$X_4$) and the Adjusted Rand Index <span class="co">[</span><span class="ot">@Rand_1971; @HubertArabie1985</span><span class="co">]</span></span>
<span id="cb9-371"><a href="#cb9-371" aria-hidden="true" tabindex="-1"></a>obtained with the selected variables in comparison to those obtained</span>
<span id="cb9-372"><a href="#cb9-372" aria-hidden="true" tabindex="-1"></a>with the full set of variables and with the true clustering variables.</span>
<span id="cb9-373"><a href="#cb9-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-374"><a href="#cb9-374" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fullpoisson, cache = TRUE}</span></span>
<span id="cb9-375"><a href="#cb9-375" aria-hidden="true" tabindex="-1"></a><span class="in">Gmax &lt;- 10</span></span>
<span id="cb9-376"><a href="#cb9-376" aria-hidden="true" tabindex="-1"></a><span class="in">fit_all &lt;- poissonmix_all(x, G = 1:Gmax)</span></span>
<span id="cb9-377"><a href="#cb9-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-378"><a href="#cb9-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-379"><a href="#cb9-379" aria-hidden="true" tabindex="-1"></a>The independent Poisson mixture model was fitted to the simulated data</span>
<span id="cb9-380"><a href="#cb9-380" aria-hidden="true" tabindex="-1"></a>with $N=$ <span class="in">`r N`</span> rows and $P=$ <span class="in">`r P`</span> columns. Models with $G=1$ to $G=$</span>
<span id="cb9-381"><a href="#cb9-381" aria-hidden="true" tabindex="-1"></a><span class="in">`r Gmax`</span> were fitted using the EM algorithm.</span>
<span id="cb9-382"><a href="#cb9-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-383"><a href="#cb9-383" aria-hidden="true" tabindex="-1"></a>The values of BIC for the independent Poisson mixture model are plotted</span>
<span id="cb9-384"><a href="#cb9-384" aria-hidden="true" tabindex="-1"></a>in @fig-BIC.</span>
<span id="cb9-385"><a href="#cb9-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-386"><a href="#cb9-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plotBIC}</span></span>
<span id="cb9-387"><a href="#cb9-387" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-BIC</span></span>
<span id="cb9-388"><a href="#cb9-388" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-389"><a href="#cb9-389" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-390"><a href="#cb9-390" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb9-391"><a href="#cb9-391" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-392"><a href="#cb9-392" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Bayesian Information Criterion (BIC) for the independent Poisson mixture model."</span></span>
<span id="cb9-393"><a href="#cb9-393" aria-hidden="true" tabindex="-1"></a><span class="in">plot(x = 1:Gmax, y = fit_all$BIC, type = "o", xlab = "G", ylab = "BIC")</span></span>
<span id="cb9-394"><a href="#cb9-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-395"><a href="#cb9-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-396"><a href="#cb9-396" aria-hidden="true" tabindex="-1"></a>The model with the highest BIC has $G=$ <span class="in">`r fit_all$bestfit$G`</span> components</span>
<span id="cb9-397"><a href="#cb9-397" aria-hidden="true" tabindex="-1"></a>and the resulting estimates of $\tau$ and $\lambda$ are given as:</span>
<span id="cb9-398"><a href="#cb9-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-399"><a href="#cb9-399" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fullpoissonparam}</span></span>
<span id="cb9-400"><a href="#cb9-400" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-params</span></span>
<span id="cb9-401"><a href="#cb9-401" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-402"><a href="#cb9-402" aria-hidden="true" tabindex="-1"></a><span class="in">#| out-width: "70%"</span></span>
<span id="cb9-403"><a href="#cb9-403" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: "Estimates of the mixing proportions and component parameters."</span></span>
<span id="cb9-404"><a href="#cb9-404" aria-hidden="true" tabindex="-1"></a><span class="in">tauhat &lt;- fit_all$bestfit$tau</span></span>
<span id="cb9-405"><a href="#cb9-405" aria-hidden="true" tabindex="-1"></a><span class="in">lambdahat &lt;- fit_all$bestfit$lambda</span></span>
<span id="cb9-406"><a href="#cb9-406" aria-hidden="true" tabindex="-1"></a><span class="in">Ghat &lt;- fit_all$bestfit$G</span></span>
<span id="cb9-407"><a href="#cb9-407" aria-hidden="true" tabindex="-1"></a><span class="in">res &lt;- data.frame(tauhat,lambdahat)</span></span>
<span id="cb9-408"><a href="#cb9-408" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(res) &lt;- c("$\\tau_g$", paste("$\\lambda_{g", 1:P, "}$", sep =""))</span></span>
<span id="cb9-409"><a href="#cb9-409" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(res) &lt;- paste("$g=", 1:Ghat, "$", sep="")</span></span>
<span id="cb9-410"><a href="#cb9-410" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::kable(res, digits = 2)</span></span>
<span id="cb9-411"><a href="#cb9-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-412"><a href="#cb9-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-413"><a href="#cb9-413" aria-hidden="true" tabindex="-1"></a>A look at @tbl-ParamSimul of true values allows us to say that these estimates are correct (except for label switching).</span>
<span id="cb9-414"><a href="#cb9-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-415"><a href="#cb9-415" aria-hidden="true" tabindex="-1"></a>Let start by initializing the stepwise algorithm.</span>
<span id="cb9-416"><a href="#cb9-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-417"><a href="#cb9-417" aria-hidden="true" tabindex="-1"></a><span class="in">```{r screening, cache = TRUE, echo = TRUE}</span></span>
<span id="cb9-418"><a href="#cb9-418" aria-hidden="true" tabindex="-1"></a><span class="in">fit_screen &lt;- poissonmix_screen(x, G = 1:Gmax)</span></span>
<span id="cb9-419"><a href="#cb9-419" aria-hidden="true" tabindex="-1"></a><span class="in">jchosen &lt;- fit_screen$jchosen</span></span>
<span id="cb9-420"><a href="#cb9-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-421"><a href="#cb9-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-422"><a href="#cb9-422" aria-hidden="true" tabindex="-1"></a>The variables selected by the screening procedure are</span>
<span id="cb9-423"><a href="#cb9-423" aria-hidden="true" tabindex="-1"></a>{<span class="in">`r fit_screen$jchosen`</span>}.</span>
<span id="cb9-424"><a href="#cb9-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-425"><a href="#cb9-425" aria-hidden="true" tabindex="-1"></a>Now, we execute the stepwise selection algorithm:</span>
<span id="cb9-426"><a href="#cb9-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-427"><a href="#cb9-427" aria-hidden="true" tabindex="-1"></a><span class="in">```{r varsel, cache = TRUE, echo = TRUE}</span></span>
<span id="cb9-428"><a href="#cb9-428" aria-hidden="true" tabindex="-1"></a><span class="in">fit &lt;- poissonmix_varsel(x, jchosen=jchosen, G = 1:Gmax)</span></span>
<span id="cb9-429"><a href="#cb9-429" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-430"><a href="#cb9-430" aria-hidden="true" tabindex="-1"></a>Note that the computing time is about 5 minutes on a laptop with 2.3 GHz Intel Core i7 processor and 32Go of RAM.</span>
<span id="cb9-431"><a href="#cb9-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-432"><a href="#cb9-432" aria-hidden="true" tabindex="-1"></a>The final chosen variables are {<span class="in">`r fit$jchosen`</span>}.</span>
<span id="cb9-433"><a href="#cb9-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-434"><a href="#cb9-434" aria-hidden="true" tabindex="-1"></a><span class="in">```{r comparison, cache = TRUE, echo = FALSE}</span></span>
<span id="cb9-435"><a href="#cb9-435" aria-hidden="true" tabindex="-1"></a><span class="in">fit_sel &lt;- poissonmix_all(x[,fit$jchosen], G = Ghat)</span></span>
<span id="cb9-436"><a href="#cb9-436" aria-hidden="true" tabindex="-1"></a><span class="in">fit_true &lt;- poissonmix_all(x[,1:4], G = Ghat)</span></span>
<span id="cb9-437"><a href="#cb9-437" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-438"><a href="#cb9-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-439"><a href="#cb9-439" aria-hidden="true" tabindex="-1"></a>Finally, the ARI obtained with the selected variables, which turn out to</span>
<span id="cb9-440"><a href="#cb9-440" aria-hidden="true" tabindex="-1"></a>be the true clustering variable, is</span>
<span id="cb9-441"><a href="#cb9-441" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(adjustedRandIndex(l0, fit_sel$bestfit$classification), 3)`</span></span>
<span id="cb9-442"><a href="#cb9-442" aria-hidden="true" tabindex="-1"></a>whereas it is</span>
<span id="cb9-443"><a href="#cb9-443" aria-hidden="true" tabindex="-1"></a><span class="in">`r round(adjustedRandIndex(l0, fit_all$bestfit$classification), 3)`</span> with</span>
<span id="cb9-444"><a href="#cb9-444" aria-hidden="true" tabindex="-1"></a>all the variables.</span>
<span id="cb9-445"><a href="#cb9-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-446"><a href="#cb9-446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Scenarios of simulation {#sec-Scenari}</span></span>
<span id="cb9-447"><a href="#cb9-447" aria-hidden="true" tabindex="-1"></a>In this section the three scenario of simulation are described.</span>
<span id="cb9-448"><a href="#cb9-448" aria-hidden="true" tabindex="-1"></a>The first scenario is similar to the previous illustrative example.</span>
<span id="cb9-449"><a href="#cb9-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-450"><a href="#cb9-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{r simulationsize, cache = TRUE, echo = FALSE}</span></span>
<span id="cb9-451"><a href="#cb9-451" aria-hidden="true" tabindex="-1"></a><span class="in">D &lt;- 100</span></span>
<span id="cb9-452"><a href="#cb9-452" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-453"><a href="#cb9-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-454"><a href="#cb9-454" aria-hidden="true" tabindex="-1"></a><span class="in">```{r simulation1whole, cache = TRUE, echo = FALSE, eval=FALSE}</span></span>
<span id="cb9-455"><a href="#cb9-455" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables &lt;- matrix(0, D, P)</span></span>
<span id="cb9-456"><a href="#cb9-456" aria-hidden="true" tabindex="-1"></a><span class="in">ARI &lt;- matrix(NA, D, 3)</span></span>
<span id="cb9-457"><a href="#cb9-457" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(ARI) &lt;- c("Selected variables", "All variables", "True variables")</span></span>
<span id="cb9-458"><a href="#cb9-458" aria-hidden="true" tabindex="-1"></a><span class="in"># register previous results</span></span>
<span id="cb9-459"><a href="#cb9-459" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables[1, fit$jchosen] &lt;- 1</span></span>
<span id="cb9-460"><a href="#cb9-460" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[1, 1] &lt;- adjustedRandIndex(l0, fit_sel$bestfit$classification)</span></span>
<span id="cb9-461"><a href="#cb9-461" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[1, 2] &lt;- adjustedRandIndex(l0, fit_all$bestfit$classification)</span></span>
<span id="cb9-462"><a href="#cb9-462" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[1, 3] &lt;- adjustedRandIndex(l0, fit_true$bestfit$classification)</span></span>
<span id="cb9-463"><a href="#cb9-463" aria-hidden="true" tabindex="-1"></a><span class="in"># loop on the data sets</span></span>
<span id="cb9-464"><a href="#cb9-464" aria-hidden="true" tabindex="-1"></a><span class="in">for (d in 2:D){</span></span>
<span id="cb9-465"><a href="#cb9-465" aria-hidden="true" tabindex="-1"></a><span class="in"># Generate data</span></span>
<span id="cb9-466"><a href="#cb9-466" aria-hidden="true" tabindex="-1"></a><span class="in">l0 &lt;- sample (1:G, prob=tau0, replace = TRUE, size = N)</span></span>
<span id="cb9-467"><a href="#cb9-467" aria-hidden="true" tabindex="-1"></a><span class="in">Ng &lt;- table(l0)</span></span>
<span id="cb9-468"><a href="#cb9-468" aria-hidden="true" tabindex="-1"></a><span class="in">Z0 &lt;- unmap(l0)</span></span>
<span id="cb9-469"><a href="#cb9-469" aria-hidden="true" tabindex="-1"></a><span class="in">x &lt;- matrix(NA, N, P)</span></span>
<span id="cb9-470"><a href="#cb9-470" aria-hidden="true" tabindex="-1"></a><span class="in">for (g in 1:G)</span></span>
<span id="cb9-471"><a href="#cb9-471" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb9-472"><a href="#cb9-472" aria-hidden="true" tabindex="-1"></a><span class="in">  x[l0==g, 1:4] &lt;- matrix(rpois(4 * Ng[g], lambda0[g, ]), Ng[g], 4, byrow = TRUE)</span></span>
<span id="cb9-473"><a href="#cb9-473" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-474"><a href="#cb9-474" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,5] &lt;- rpois(N, exp(0.2 * x[, 2]))</span></span>
<span id="cb9-475"><a href="#cb9-475" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,6] &lt;- rpois(N, exp(x[, 1] * 0.2 - 0.1 * x[, 2]))</span></span>
<span id="cb9-476"><a href="#cb9-476" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,7] &lt;- rpois(N, exp(0.1 * (x[, 1] + x[, 3] + x[, 4])))</span></span>
<span id="cb9-477"><a href="#cb9-477" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,8] &lt;- rpois(N, 4)</span></span>
<span id="cb9-478"><a href="#cb9-478" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,9] &lt;- rpois(N, 2)</span></span>
<span id="cb9-479"><a href="#cb9-479" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,10] &lt;- rpois(N, 1)</span></span>
<span id="cb9-480"><a href="#cb9-480" aria-hidden="true" tabindex="-1"></a><span class="in"># Select G</span></span>
<span id="cb9-481"><a href="#cb9-481" aria-hidden="true" tabindex="-1"></a><span class="in">fit_all &lt;- poissonmix_all(x, G = 1:Gmax)</span></span>
<span id="cb9-482"><a href="#cb9-482" aria-hidden="true" tabindex="-1"></a><span class="in">Ghat &lt;- fit_all$bestfit$G</span></span>
<span id="cb9-483"><a href="#cb9-483" aria-hidden="true" tabindex="-1"></a><span class="in"># Select initial variable</span></span>
<span id="cb9-484"><a href="#cb9-484" aria-hidden="true" tabindex="-1"></a><span class="in">fit_screen &lt;- poissonmix_screen(x, G = 1:Gmax)</span></span>
<span id="cb9-485"><a href="#cb9-485" aria-hidden="true" tabindex="-1"></a><span class="in">jchosen &lt;- fit_screen$jchosen</span></span>
<span id="cb9-486"><a href="#cb9-486" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable selection</span></span>
<span id="cb9-487"><a href="#cb9-487" aria-hidden="true" tabindex="-1"></a><span class="in">fit &lt;- poissonmix_varsel(x, jchosen=jchosen, G = 1:Gmax)</span></span>
<span id="cb9-488"><a href="#cb9-488" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables[d,fit$jchosen] &lt;- 1</span></span>
<span id="cb9-489"><a href="#cb9-489" aria-hidden="true" tabindex="-1"></a><span class="in"># Computing partitions and ARI</span></span>
<span id="cb9-490"><a href="#cb9-490" aria-hidden="true" tabindex="-1"></a><span class="in">fit_sel &lt;- poissonmix_all(x[,fit$jchosen], G = Ghat)</span></span>
<span id="cb9-491"><a href="#cb9-491" aria-hidden="true" tabindex="-1"></a><span class="in">fit_true &lt;- poissonmix_all(x[,1:4], G = Ghat)</span></span>
<span id="cb9-492"><a href="#cb9-492" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d, 1] &lt;- adjustedRandIndex(l0, fit_sel$bestfit$classification)</span></span>
<span id="cb9-493"><a href="#cb9-493" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d, 2] &lt;- adjustedRandIndex(l0, fit_all$bestfit$classification)</span></span>
<span id="cb9-494"><a href="#cb9-494" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d, 3] &lt;- adjustedRandIndex(l0, fit_true$bestfit$classification)</span></span>
<span id="cb9-495"><a href="#cb9-495" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-496"><a href="#cb9-496" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-497"><a href="#cb9-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-498"><a href="#cb9-498" aria-hidden="true" tabindex="-1"></a><span class="in">```{r loadsim1, echo = FALSE, message = FALSE}</span></span>
<span id="cb9-499"><a href="#cb9-499" aria-hidden="true" tabindex="-1"></a><span class="in">load('result-simulation1.Rdata')</span></span>
<span id="cb9-500"><a href="#cb9-500" aria-hidden="true" tabindex="-1"></a><span class="in">ARI1=ARI</span></span>
<span id="cb9-501"><a href="#cb9-501" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables1=selected_variables</span></span>
<span id="cb9-502"><a href="#cb9-502" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-503"><a href="#cb9-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-504"><a href="#cb9-504" aria-hidden="true" tabindex="-1"></a>The second scenario is similar to the first one, except for variables</span>
<span id="cb9-505"><a href="#cb9-505" aria-hidden="true" tabindex="-1"></a>$X_5$, $X_6$ and $X_7$ which are still redundant but linked to the true</span>
<span id="cb9-506"><a href="#cb9-506" aria-hidden="true" tabindex="-1"></a>clustering variables through linear, quadratic and exponential term in an identity link function, respectively, and not a Poisson GLM with logarithm link function. More precisely, $X_5$, $X_6$ and $X_7$ have Poisson distribution of respective parameter $\lambda_{g5}=\exp(2X_2)$, $\lambda_{g6}=\exp(X_1^2+X_3)$ and $\lambda_{g7}=\exp(\exp(0.1 (X_1 + X_3+ X_4)))$.</span>
<span id="cb9-507"><a href="#cb9-507" aria-hidden="true" tabindex="-1"></a>Thus, the data are simulated from a model which</span>
<span id="cb9-508"><a href="#cb9-508" aria-hidden="true" tabindex="-1"></a>does not satisfy assumptions of model ${\cal M}_2$.</span>
<span id="cb9-509"><a href="#cb9-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-510"><a href="#cb9-510" aria-hidden="true" tabindex="-1"></a><span class="in">```{r simulation2whole, cache = TRUE, echo = FALSE, eval = FALSE}</span></span>
<span id="cb9-511"><a href="#cb9-511" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables &lt;- matrix(0, D, P)</span></span>
<span id="cb9-512"><a href="#cb9-512" aria-hidden="true" tabindex="-1"></a><span class="in">ARI &lt;- matrix(NA, D, 3)</span></span>
<span id="cb9-513"><a href="#cb9-513" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(ARI) &lt;- c("Selected variables", "All variables", "True variables")</span></span>
<span id="cb9-514"><a href="#cb9-514" aria-hidden="true" tabindex="-1"></a><span class="in"># loop on the data sets</span></span>
<span id="cb9-515"><a href="#cb9-515" aria-hidden="true" tabindex="-1"></a><span class="in">for (d in 1:D){</span></span>
<span id="cb9-516"><a href="#cb9-516" aria-hidden="true" tabindex="-1"></a><span class="in"># Generate data</span></span>
<span id="cb9-517"><a href="#cb9-517" aria-hidden="true" tabindex="-1"></a><span class="in">l0 &lt;- sample (1:G, prob=tau0, replace = TRUE, size = N)</span></span>
<span id="cb9-518"><a href="#cb9-518" aria-hidden="true" tabindex="-1"></a><span class="in">Ng &lt;- table(l0)</span></span>
<span id="cb9-519"><a href="#cb9-519" aria-hidden="true" tabindex="-1"></a><span class="in">Z0 &lt;- unmap(l0)</span></span>
<span id="cb9-520"><a href="#cb9-520" aria-hidden="true" tabindex="-1"></a><span class="in">x &lt;- matrix(NA, N, P)</span></span>
<span id="cb9-521"><a href="#cb9-521" aria-hidden="true" tabindex="-1"></a><span class="in">for (g in 1:G)</span></span>
<span id="cb9-522"><a href="#cb9-522" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb9-523"><a href="#cb9-523" aria-hidden="true" tabindex="-1"></a><span class="in">  x[l0==g, 1:4] &lt;- matrix(rpois(4 * Ng[g], lambda0[g, ]), Ng[g], 4, byrow = TRUE)</span></span>
<span id="cb9-524"><a href="#cb9-524" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-525"><a href="#cb9-525" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,5] &lt;- rpois(N, 2 * x[, 2])</span></span>
<span id="cb9-526"><a href="#cb9-526" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,6] &lt;- rpois(N, x[, 1] ^ 2 + x[, 3])</span></span>
<span id="cb9-527"><a href="#cb9-527" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,7] &lt;- rpois(N, exp(0.1 * (x[, 1] + x[, 3] + x[, 4])))</span></span>
<span id="cb9-528"><a href="#cb9-528" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,8] &lt;- rpois(N, 4)</span></span>
<span id="cb9-529"><a href="#cb9-529" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,9] &lt;- rpois(N, 2)</span></span>
<span id="cb9-530"><a href="#cb9-530" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,10] &lt;- rpois(N, 1)</span></span>
<span id="cb9-531"><a href="#cb9-531" aria-hidden="true" tabindex="-1"></a><span class="in"># Select G</span></span>
<span id="cb9-532"><a href="#cb9-532" aria-hidden="true" tabindex="-1"></a><span class="in">fit_all &lt;- poissonmix_all(x, G = 1:Gmax)</span></span>
<span id="cb9-533"><a href="#cb9-533" aria-hidden="true" tabindex="-1"></a><span class="in">Ghat &lt;- fit_all$bestfit$G</span></span>
<span id="cb9-534"><a href="#cb9-534" aria-hidden="true" tabindex="-1"></a><span class="in"># Select initial variable</span></span>
<span id="cb9-535"><a href="#cb9-535" aria-hidden="true" tabindex="-1"></a><span class="in">fit_screen &lt;- poissonmix_screen(x, G = 1:Gmax) </span></span>
<span id="cb9-536"><a href="#cb9-536" aria-hidden="true" tabindex="-1"></a><span class="in">jchosen &lt;- fit_screen$jchosen</span></span>
<span id="cb9-537"><a href="#cb9-537" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable selection</span></span>
<span id="cb9-538"><a href="#cb9-538" aria-hidden="true" tabindex="-1"></a><span class="in">fit &lt;- poissonmix_varsel(x, jchosen=jchosen, G = 1:Gmax)</span></span>
<span id="cb9-539"><a href="#cb9-539" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables[d,fit$jchosen]=1</span></span>
<span id="cb9-540"><a href="#cb9-540" aria-hidden="true" tabindex="-1"></a><span class="in"># Computing partitions and ARI</span></span>
<span id="cb9-541"><a href="#cb9-541" aria-hidden="true" tabindex="-1"></a><span class="in">fit_sel &lt;- poissonmix_all(x[,fit$jchosen], G = Ghat)</span></span>
<span id="cb9-542"><a href="#cb9-542" aria-hidden="true" tabindex="-1"></a><span class="in">fit_true &lt;- poissonmix_all(x[,1:4], G = Ghat)</span></span>
<span id="cb9-543"><a href="#cb9-543" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d, 1] &lt;- adjustedRandIndex(l0, fit_sel$bestfit$classification)</span></span>
<span id="cb9-544"><a href="#cb9-544" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d, 2] &lt;- adjustedRandIndex(l0, fit_all$bestfit$classification)</span></span>
<span id="cb9-545"><a href="#cb9-545" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d, 3] &lt;- adjustedRandIndex(l0, fit_true$bestfit$classification)</span></span>
<span id="cb9-546"><a href="#cb9-546" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-547"><a href="#cb9-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-548"><a href="#cb9-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-549"><a href="#cb9-549" aria-hidden="true" tabindex="-1"></a><span class="in">```{r loadsim2, echo = FALSE, message = FALSE}</span></span>
<span id="cb9-550"><a href="#cb9-550" aria-hidden="true" tabindex="-1"></a><span class="in">load('result-simulation2.Rdata')</span></span>
<span id="cb9-551"><a href="#cb9-551" aria-hidden="true" tabindex="-1"></a><span class="in">ARI2=ARI</span></span>
<span id="cb9-552"><a href="#cb9-552" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables2=selected_variables</span></span>
<span id="cb9-553"><a href="#cb9-553" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-554"><a href="#cb9-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-555"><a href="#cb9-555" aria-hidden="true" tabindex="-1"></a>The third scenario is similar to the second one, but some dependence</span>
<span id="cb9-556"><a href="#cb9-556" aria-hidden="true" tabindex="-1"></a>between the clustering variables $X_1$ and $X_2$ is introduced, in order</span>
<span id="cb9-557"><a href="#cb9-557" aria-hidden="true" tabindex="-1"></a>to create some redundancy among the true clustering variables. For this, $X_1$ and $X_2$ are simulated as in the previous setting, and a same term is added to both of these variables (simulated according a Poisson distribution of parameter 2) .</span>
<span id="cb9-558"><a href="#cb9-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-559"><a href="#cb9-559" aria-hidden="true" tabindex="-1"></a><span class="in">```{r simulation3whole, cache = TRUE, echo = FALSE,eval = FALSE}</span></span>
<span id="cb9-560"><a href="#cb9-560" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables &lt;- matrix(0,D,P)</span></span>
<span id="cb9-561"><a href="#cb9-561" aria-hidden="true" tabindex="-1"></a><span class="in">ARI &lt;- matrix(NA,D,3)</span></span>
<span id="cb9-562"><a href="#cb9-562" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(ARI) &lt;- c("Selected variables", "All variables", "True variables")</span></span>
<span id="cb9-563"><a href="#cb9-563" aria-hidden="true" tabindex="-1"></a><span class="in"># loop on the data sets</span></span>
<span id="cb9-564"><a href="#cb9-564" aria-hidden="true" tabindex="-1"></a><span class="in">for (d in 1:D){</span></span>
<span id="cb9-565"><a href="#cb9-565" aria-hidden="true" tabindex="-1"></a><span class="in"># Generate data</span></span>
<span id="cb9-566"><a href="#cb9-566" aria-hidden="true" tabindex="-1"></a><span class="in">l0 &lt;- sample (1:G, prob=tau0, replace = TRUE, size = N)</span></span>
<span id="cb9-567"><a href="#cb9-567" aria-hidden="true" tabindex="-1"></a><span class="in">Ng &lt;- table(l0)</span></span>
<span id="cb9-568"><a href="#cb9-568" aria-hidden="true" tabindex="-1"></a><span class="in">Z0 &lt;- unmap(l0)</span></span>
<span id="cb9-569"><a href="#cb9-569" aria-hidden="true" tabindex="-1"></a><span class="in">x &lt;- matrix(NA, N, P)</span></span>
<span id="cb9-570"><a href="#cb9-570" aria-hidden="true" tabindex="-1"></a><span class="in">for (g in 1:G)</span></span>
<span id="cb9-571"><a href="#cb9-571" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb9-572"><a href="#cb9-572" aria-hidden="true" tabindex="-1"></a><span class="in">  x[l0==g, 1:4] &lt;- matrix(rpois(4 * Ng[g], lambda0[g, ]), Ng[g], 4, byrow = TRUE)</span></span>
<span id="cb9-573"><a href="#cb9-573" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-574"><a href="#cb9-574" aria-hidden="true" tabindex="-1"></a><span class="in">x[, 1:2] &lt;- x[, 1:2] + rpois(N, 2)</span></span>
<span id="cb9-575"><a href="#cb9-575" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,5] &lt;- rpois(N, 2 * x[, 2])</span></span>
<span id="cb9-576"><a href="#cb9-576" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,6] &lt;- rpois(N, x[, 1]^2 + x[, 3])</span></span>
<span id="cb9-577"><a href="#cb9-577" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,7] &lt;- rpois(N, exp(0.1 * (x[, 1] + x[, 3] + x[, 4])))</span></span>
<span id="cb9-578"><a href="#cb9-578" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,8] &lt;- rpois(N, 4)</span></span>
<span id="cb9-579"><a href="#cb9-579" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,9] &lt;- rpois(N, 2)</span></span>
<span id="cb9-580"><a href="#cb9-580" aria-hidden="true" tabindex="-1"></a><span class="in">x[ ,10] &lt;- rpois(N, 1)</span></span>
<span id="cb9-581"><a href="#cb9-581" aria-hidden="true" tabindex="-1"></a><span class="in"># Select G</span></span>
<span id="cb9-582"><a href="#cb9-582" aria-hidden="true" tabindex="-1"></a><span class="in">fit_all &lt;- poissonmix_all(x, G = 1:Gmax)</span></span>
<span id="cb9-583"><a href="#cb9-583" aria-hidden="true" tabindex="-1"></a><span class="in">Ghat &lt;- fit_all$bestfit$G</span></span>
<span id="cb9-584"><a href="#cb9-584" aria-hidden="true" tabindex="-1"></a><span class="in"># Select initial variable</span></span>
<span id="cb9-585"><a href="#cb9-585" aria-hidden="true" tabindex="-1"></a><span class="in">fit_screen &lt;- poissonmix_screen(x, G = 1:Gmax) # JJ: why to not use Ghat ??</span></span>
<span id="cb9-586"><a href="#cb9-586" aria-hidden="true" tabindex="-1"></a><span class="in">jchosen &lt;- fit_screen$jchosen</span></span>
<span id="cb9-587"><a href="#cb9-587" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable selection</span></span>
<span id="cb9-588"><a href="#cb9-588" aria-hidden="true" tabindex="-1"></a><span class="in">fit &lt;- poissonmix_varsel(x, jchosen=jchosen, G = 1:Gmax)</span></span>
<span id="cb9-589"><a href="#cb9-589" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables[d,fit$jchosen]=1</span></span>
<span id="cb9-590"><a href="#cb9-590" aria-hidden="true" tabindex="-1"></a><span class="in"># Computing partitions and ARI</span></span>
<span id="cb9-591"><a href="#cb9-591" aria-hidden="true" tabindex="-1"></a><span class="in">fit_sel &lt;- poissonmix_all(x[,fit$jchosen], G = Ghat)</span></span>
<span id="cb9-592"><a href="#cb9-592" aria-hidden="true" tabindex="-1"></a><span class="in">fit_true &lt;- poissonmix_all(x[,1:4], G = Ghat)</span></span>
<span id="cb9-593"><a href="#cb9-593" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d,1] &lt;- adjustedRandIndex(l0, fit_sel$bestfit$classification)</span></span>
<span id="cb9-594"><a href="#cb9-594" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d,2] &lt;- adjustedRandIndex(l0, fit_all$bestfit$classification)</span></span>
<span id="cb9-595"><a href="#cb9-595" aria-hidden="true" tabindex="-1"></a><span class="in">ARI[d,3] &lt;- adjustedRandIndex(l0, fit_true$bestfit$classification)</span></span>
<span id="cb9-596"><a href="#cb9-596" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-597"><a href="#cb9-597" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-598"><a href="#cb9-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-599"><a href="#cb9-599" aria-hidden="true" tabindex="-1"></a><span class="in">```{r loadsim3, echo = FALSE, message = FALSE}</span></span>
<span id="cb9-600"><a href="#cb9-600" aria-hidden="true" tabindex="-1"></a><span class="in">load('result-simulation3.Rdata')</span></span>
<span id="cb9-601"><a href="#cb9-601" aria-hidden="true" tabindex="-1"></a><span class="in">ARI3=ARI</span></span>
<span id="cb9-602"><a href="#cb9-602" aria-hidden="true" tabindex="-1"></a><span class="in">selected_variables3=selected_variables</span></span>
<span id="cb9-603"><a href="#cb9-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-604"><a href="#cb9-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-605"><a href="#cb9-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-606"><a href="#cb9-606" aria-hidden="true" tabindex="-1"></a><span class="fu">## Results {#sec-wholeresults}</span></span>
<span id="cb9-607"><a href="#cb9-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-608"><a href="#cb9-608" aria-hidden="true" tabindex="-1"></a>@tbl-HISTall shows the number of times, among the <span class="in">`r D`</span> simulated</span>
<span id="cb9-609"><a href="#cb9-609" aria-hidden="true" tabindex="-1"></a>data sets, that each variable is selected. </span>
<span id="cb9-610"><a href="#cb9-610" aria-hidden="true" tabindex="-1"></a>For Scenario 1, the model selection procedure perform perfectly, selecting each time only the true clustering</span>
<span id="cb9-611"><a href="#cb9-611" aria-hidden="true" tabindex="-1"></a>variables. For Scenario 2, due to the fact the link between the redundant and the true clustering</span>
<span id="cb9-612"><a href="#cb9-612" aria-hidden="true" tabindex="-1"></a>variables is not a standard Poisson GLM, the variable selection is perturbed and</span>
<span id="cb9-613"><a href="#cb9-613" aria-hidden="true" tabindex="-1"></a>variables $X_5$ is sometimes selected. For Scenario 3, the results is that the dependency between $X_1$ and $X_2$ perturb the variable selection, and only one of them is selected (and even sometimes</span>
<span id="cb9-614"><a href="#cb9-614" aria-hidden="true" tabindex="-1"></a>none of them). Redundant variables $X_5$ and $X_6$, which are linked to the clustering variables but with a linear link, are also sometimes selected. </span>
<span id="cb9-615"><a href="#cb9-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{r tabHISTall}</span></span>
<span id="cb9-616"><a href="#cb9-616" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-HISTall</span></span>
<span id="cb9-617"><a href="#cb9-617" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-618"><a href="#cb9-618" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-619"><a href="#cb9-619" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-620"><a href="#cb9-620" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: "Number of selection for each variable, simulation setting number 3."</span></span>
<span id="cb9-621"><a href="#cb9-621" aria-hidden="true" tabindex="-1"></a><span class="in">res &lt;- matrix(c(colSums(selected_variables1),colSums(selected_variables2),colSums(selected_variables3)),3,P,byrow = TRUE)</span></span>
<span id="cb9-622"><a href="#cb9-622" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(res) &lt;- paste("$X_{", 1:P, "}$", sep ="")</span></span>
<span id="cb9-623"><a href="#cb9-623" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(res) &lt;- c("Scenario 1","Scenario 2","Scenario 3")</span></span>
<span id="cb9-624"><a href="#cb9-624" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::kable(res)</span></span>
<span id="cb9-625"><a href="#cb9-625" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-626"><a href="#cb9-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-627"><a href="#cb9-627" aria-hidden="true" tabindex="-1"></a>@fig-all3 plots the distribution of the ARI differences between the model with either the selected variables or all the variables, and the one with the true clustering variables. These plots shows that for all scenarios, the ARI of the model with the selected variables (left boxplot of each plot) are always closest to the optimal ARI (obtained with the true clustering variables).</span>
<span id="cb9-628"><a href="#cb9-628" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plotall3}</span></span>
<span id="cb9-629"><a href="#cb9-629" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-all3</span></span>
<span id="cb9-630"><a href="#cb9-630" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-631"><a href="#cb9-631" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-632"><a href="#cb9-632" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-633"><a href="#cb9-633" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Distribution of the ARI differences with the model with the true clustering variables, for the model with the selected variables and the model with all variables."</span></span>
<span id="cb9-634"><a href="#cb9-634" aria-hidden="true" tabindex="-1"></a><span class="in">par(mfrow = c(1, 3))</span></span>
<span id="cb9-635"><a href="#cb9-635" aria-hidden="true" tabindex="-1"></a><span class="in">par(mar = c(6, 2, 2, 2))</span></span>
<span id="cb9-636"><a href="#cb9-636" aria-hidden="true" tabindex="-1"></a><span class="in">ARI=matrix(c(ARI1[,1]-ARI1[,3],ARI1[,2]-ARI1[,3]),ncol=2,byrow=FALSE)</span></span>
<span id="cb9-637"><a href="#cb9-637" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(ARI)&lt;-c("Selected var. - True var.","All var. - True var.")</span></span>
<span id="cb9-638"><a href="#cb9-638" aria-hidden="true" tabindex="-1"></a><span class="in">boxplot(ARI, cex.axis=0.7,ylim=c(-.5,.2),las = 2,main='Scenario 1',xlab='');abline(h=0,col=2,las = 2)</span></span>
<span id="cb9-639"><a href="#cb9-639" aria-hidden="true" tabindex="-1"></a><span class="in">ARI=matrix(c(ARI2[,1]-ARI2[,3],ARI2[,2]-ARI2[,3]),ncol=2,byrow=FALSE)</span></span>
<span id="cb9-640"><a href="#cb9-640" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(ARI)&lt;-c("Selected var. - True var.","All var. - True var.")</span></span>
<span id="cb9-641"><a href="#cb9-641" aria-hidden="true" tabindex="-1"></a><span class="in">boxplot(ARI, cex.axis=0.7,ylim=c(-.5,.2),las = 2,main='Scenario 21',xlab='');abline(h=0,col=2,las = 2)</span></span>
<span id="cb9-642"><a href="#cb9-642" aria-hidden="true" tabindex="-1"></a><span class="in">ARI=matrix(c(ARI3[,1]-ARI3[,3],ARI3[,2]-ARI3[,3]),ncol=2,byrow=FALSE)</span></span>
<span id="cb9-643"><a href="#cb9-643" aria-hidden="true" tabindex="-1"></a><span class="in">colnames(ARI)&lt;-c("Selected var. - True var.","All var. - True var.")</span></span>
<span id="cb9-644"><a href="#cb9-644" aria-hidden="true" tabindex="-1"></a><span class="in">boxplot(ARI, cex.axis=0.7,ylim=c(-.5,.2),las = 2,main='Scenario 3',xlab='');abline(h=0,col=2,las = 2)</span></span>
<span id="cb9-645"><a href="#cb9-645" aria-hidden="true" tabindex="-1"></a><span class="in">par(mfrow = c(1, 1))</span></span>
<span id="cb9-646"><a href="#cb9-646" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-647"><a href="#cb9-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-648"><a href="#cb9-648" aria-hidden="true" tabindex="-1"></a>Finally @fig-all2 plots the histogram of the difference of ARI with the selected variables and with all the variables.</span>
<span id="cb9-649"><a href="#cb9-649" aria-hidden="true" tabindex="-1"></a>This plot illustrates the interest of variable selection on the clustering results, and indeed, for all the scenarios, the ARI is better with the selected variables than when using all the variables. </span>
<span id="cb9-650"><a href="#cb9-650" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plotall2}</span></span>
<span id="cb9-651"><a href="#cb9-651" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-all2</span></span>
<span id="cb9-652"><a href="#cb9-652" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-653"><a href="#cb9-653" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-654"><a href="#cb9-654" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-655"><a href="#cb9-655" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Distribution of the ARI differences for the model with the selected variables and the model with all variables."</span></span>
<span id="cb9-656"><a href="#cb9-656" aria-hidden="true" tabindex="-1"></a><span class="in">par(mfrow = c(1, 3))</span></span>
<span id="cb9-657"><a href="#cb9-657" aria-hidden="true" tabindex="-1"></a><span class="in">par(mar = c(6, 2, 2, 2))</span></span>
<span id="cb9-658"><a href="#cb9-658" aria-hidden="true" tabindex="-1"></a><span class="in">hist(ARI1[, 1] - ARI1[, 2], xlab = 'ARI differences',main='Scenario 1',xlim=c(-.1,.5))</span></span>
<span id="cb9-659"><a href="#cb9-659" aria-hidden="true" tabindex="-1"></a><span class="in">abline(v=0,col=2)</span></span>
<span id="cb9-660"><a href="#cb9-660" aria-hidden="true" tabindex="-1"></a><span class="in">hist(ARI2[, 1] - ARI2[, 2], xlab = 'ARI differences',main='Scenario 2',xlim=c(-.1,.5))</span></span>
<span id="cb9-661"><a href="#cb9-661" aria-hidden="true" tabindex="-1"></a><span class="in">abline(v=0,col=2)</span></span>
<span id="cb9-662"><a href="#cb9-662" aria-hidden="true" tabindex="-1"></a><span class="in">hist(ARI3[, 1] - ARI3[, 2], xlab = 'ARI differences',main='Scenario 3',xlim=c(-.1,.5))</span></span>
<span id="cb9-663"><a href="#cb9-663" aria-hidden="true" tabindex="-1"></a><span class="in">abline(v=0,col=2)</span></span>
<span id="cb9-664"><a href="#cb9-664" aria-hidden="true" tabindex="-1"></a><span class="in">par(mfrow = c(1, 1))</span></span>
<span id="cb9-665"><a href="#cb9-665" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-666"><a href="#cb9-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-667"><a href="#cb9-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-668"><a href="#cb9-668" aria-hidden="true" tabindex="-1"></a><span class="fu"># International Ultrarunning Association Data</span></span>
<span id="cb9-669"><a href="#cb9-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-670"><a href="#cb9-670" aria-hidden="true" tabindex="-1"></a><span class="in">```{r loadrunning, echo = FALSE, message = FALSE, warning = FALSE, fig.align='center'}</span></span>
<span id="cb9-671"><a href="#cb9-671" aria-hidden="true" tabindex="-1"></a><span class="in">load("X24H.Rdata")</span></span>
<span id="cb9-672"><a href="#cb9-672" aria-hidden="true" tabindex="-1"></a><span class="in">P &lt;- ncol(x)</span></span>
<span id="cb9-673"><a href="#cb9-673" aria-hidden="true" tabindex="-1"></a><span class="in">Gmax &lt;- 10</span></span>
<span id="cb9-674"><a href="#cb9-674" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-675"><a href="#cb9-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-676"><a href="#cb9-676" aria-hidden="true" tabindex="-1"></a>We apply the proposed procedure to the data from the 2012 International Ultrarunning Association World 24H Championships. </span>
<span id="cb9-677"><a href="#cb9-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-678"><a href="#cb9-678" aria-hidden="true" tabindex="-1"></a>We start by initializing the stepwise algorithm, and find the</span>
<span id="cb9-679"><a href="#cb9-679" aria-hidden="true" tabindex="-1"></a>variables selected by the screening procedure:</span>
<span id="cb9-680"><a href="#cb9-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-681"><a href="#cb9-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r screenrunning, cache = TRUE, echo = TRUE}</span></span>
<span id="cb9-682"><a href="#cb9-682" aria-hidden="true" tabindex="-1"></a><span class="in">fit_screen &lt;- poissonmix_screen(x, G = 1:Gmax)</span></span>
<span id="cb9-683"><a href="#cb9-683" aria-hidden="true" tabindex="-1"></a><span class="in">jchosen &lt;- fit_screen$jchosen</span></span>
<span id="cb9-684"><a href="#cb9-684" aria-hidden="true" tabindex="-1"></a><span class="in">jchosen</span></span>
<span id="cb9-685"><a href="#cb9-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-686"><a href="#cb9-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-687"><a href="#cb9-687" aria-hidden="true" tabindex="-1"></a>We then execute the proposed stepwise selection algorithm (the computing time is about 26 minutes on a laptop with 2.3 GHz Intel Core i7 processor and 32Go of RAM):</span>
<span id="cb9-688"><a href="#cb9-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-689"><a href="#cb9-689" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fitrunning, cache = TRUE, echo = TRUE, eval = FALSE}</span></span>
<span id="cb9-690"><a href="#cb9-690" aria-hidden="true" tabindex="-1"></a><span class="in">fit &lt;- poissonmix_varsel(x, jchosen = jchosen, G = 1:Gmax)</span></span>
<span id="cb9-691"><a href="#cb9-691" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-692"><a href="#cb9-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-693"><a href="#cb9-693" aria-hidden="true" tabindex="-1"></a><span class="in">```{r loadfitrunning, echo = FALSE}</span></span>
<span id="cb9-694"><a href="#cb9-694" aria-hidden="true" tabindex="-1"></a><span class="in">load("RunningFit.Rdata")</span></span>
<span id="cb9-695"><a href="#cb9-695" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-696"><a href="#cb9-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-697"><a href="#cb9-697" aria-hidden="true" tabindex="-1"></a>The final chosen variables found by the algorithm are:</span>
<span id="cb9-698"><a href="#cb9-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-699"><a href="#cb9-699" aria-hidden="true" tabindex="-1"></a><span class="in">```{r runningchosen, echo = FALSE}</span></span>
<span id="cb9-700"><a href="#cb9-700" aria-hidden="true" tabindex="-1"></a><span class="in">fit$jchosen</span></span>
<span id="cb9-701"><a href="#cb9-701" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-702"><a href="#cb9-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-703"><a href="#cb9-703" aria-hidden="true" tabindex="-1"></a>The optimal number of clusters <span class="in">`r fit$bestfit$G`</span> has been chosen inside the stepwise selection algorithm. The same choice is obtained when looking for the best $G$ with the conditionally independent Poisson mixture on the selected variables (@fig-BIC2).</span>
<span id="cb9-704"><a href="#cb9-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-705"><a href="#cb9-705" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plotBIC2}</span></span>
<span id="cb9-706"><a href="#cb9-706" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-BIC2</span></span>
<span id="cb9-707"><a href="#cb9-707" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb9-708"><a href="#cb9-708" aria-hidden="true" tabindex="-1"></a><span class="in">#| message: false</span></span>
<span id="cb9-709"><a href="#cb9-709" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb9-710"><a href="#cb9-710" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-align: "center"</span></span>
<span id="cb9-711"><a href="#cb9-711" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Bayesian Information Criterion (BIC) for the independent Poisson mixture model with the seleceted variables."</span></span>
<span id="cb9-712"><a href="#cb9-712" aria-hidden="true" tabindex="-1"></a><span class="in">Gmax &lt;- 10</span></span>
<span id="cb9-713"><a href="#cb9-713" aria-hidden="true" tabindex="-1"></a><span class="in">fit_all &lt;- poissonmix_all(as.matrix(x[,fit$jchosen]), G = 1:Gmax)</span></span>
<span id="cb9-714"><a href="#cb9-714" aria-hidden="true" tabindex="-1"></a><span class="in">plot(x = 1:Gmax, y = fit_all$BIC, type = "o", xlab = "G", ylab = "BIC")</span></span>
<span id="cb9-715"><a href="#cb9-715" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-716"><a href="#cb9-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-717"><a href="#cb9-717" aria-hidden="true" tabindex="-1"></a>In order to illustrate the results, we plot the cluster means according</span>
<span id="cb9-718"><a href="#cb9-718" aria-hidden="true" tabindex="-1"></a>to the <span class="in">`r P`</span> variable mean parameters per cluster. For each variable not</span>
<span id="cb9-719"><a href="#cb9-719" aria-hidden="true" tabindex="-1"></a>in the chosen variable set, a Poisson regression model is fitted with</span>
<span id="cb9-720"><a href="#cb9-720" aria-hidden="true" tabindex="-1"></a>the chosen variables as predictors. Forward and backwards variable</span>
<span id="cb9-721"><a href="#cb9-721" aria-hidden="true" tabindex="-1"></a>selection is conducted on this regression, if the regression model has any</span>
<span id="cb9-722"><a href="#cb9-722" aria-hidden="true" tabindex="-1"></a>predictor variables, then the variable is called "redundant" and if the</span>
<span id="cb9-723"><a href="#cb9-723" aria-hidden="true" tabindex="-1"></a>regression model has no predictor variables, then the variable is called</span>
<span id="cb9-724"><a href="#cb9-724" aria-hidden="true" tabindex="-1"></a>"irrelevant".</span>
<span id="cb9-725"><a href="#cb9-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-726"><a href="#cb9-726" aria-hidden="true" tabindex="-1"></a>@fig-Running-result1 shows the cluster mean for each variable, where the</span>
<span id="cb9-727"><a href="#cb9-727" aria-hidden="true" tabindex="-1"></a>label indicates if the variable is irrelevant for clustering ("I"),</span>
<span id="cb9-728"><a href="#cb9-728" aria-hidden="true" tabindex="-1"></a>redundant ("R") or useful (the label is then the cluster number).</span>
<span id="cb9-729"><a href="#cb9-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-730"><a href="#cb9-730" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plotrunning, echo=FALSE, fig.align='center', cache = TRUE}</span></span>
<span id="cb9-731"><a href="#cb9-731" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-Running-result1</span></span>
<span id="cb9-732"><a href="#cb9-732" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Cluster means and usefulness of the variables."</span></span>
<span id="cb9-733"><a href="#cb9-733" aria-hidden="true" tabindex="-1"></a><span class="in">matplot(fit$jchosen, t(fit$bestfit$lambda), xlab = 'Hours', ylab = "Number of laps", xaxt = "n",yaxt = "n", xlim = c(1, P), ylim = range(x[,-16]))</span></span>
<span id="cb9-734"><a href="#cb9-734" aria-hidden="true" tabindex="-1"></a><span class="in">axis(1, at = seq(6, 24, 6),labels = seq(6, 24, 6))</span></span>
<span id="cb9-735"><a href="#cb9-735" aria-hidden="true" tabindex="-1"></a><span class="in">axis(2, at = 0:9, labels = 0:9)</span></span>
<span id="cb9-736"><a href="#cb9-736" aria-hidden="true" tabindex="-1"></a><span class="in">abline(v = fit$jchosen, col = "lightgray", lty = 3)</span></span>
<span id="cb9-737"><a href="#cb9-737" aria-hidden="true" tabindex="-1"></a><span class="in">matplot(fit$jchosen, t(fit$bestfit$lambda), type = "p", add = TRUE)</span></span>
<span id="cb9-738"><a href="#cb9-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-739"><a href="#cb9-739" aria-hidden="true" tabindex="-1"></a><span class="in">G &lt;- fit$bestfit$G</span></span>
<span id="cb9-740"><a href="#cb9-740" aria-hidden="true" tabindex="-1"></a><span class="in">Z &lt;- fit$bestfit$Z</span></span>
<span id="cb9-741"><a href="#cb9-741" aria-hidden="true" tabindex="-1"></a><span class="in">x &lt;- as.matrix(x)</span></span>
<span id="cb9-742"><a href="#cb9-742" aria-hidden="true" tabindex="-1"></a><span class="in">xmeans &lt;- t(Z) %*% x / apply(Z, 2, sum)</span></span>
<span id="cb9-743"><a href="#cb9-743" aria-hidden="true" tabindex="-1"></a><span class="in">jremove &lt;- setdiff(1:P, fit$jchosen)</span></span>
<span id="cb9-744"><a href="#cb9-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-745"><a href="#cb9-745" aria-hidden="true" tabindex="-1"></a><span class="in">vartype &lt;- rep(NA, P)</span></span>
<span id="cb9-746"><a href="#cb9-746" aria-hidden="true" tabindex="-1"></a><span class="in">vartype[fit$jchosen] &lt;- "Clustering"</span></span>
<span id="cb9-747"><a href="#cb9-747" aria-hidden="true" tabindex="-1"></a><span class="in">for (j in jremove)</span></span>
<span id="cb9-748"><a href="#cb9-748" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb9-749"><a href="#cb9-749" aria-hidden="true" tabindex="-1"></a><span class="in">  y &lt;- x[, j]</span></span>
<span id="cb9-750"><a href="#cb9-750" aria-hidden="true" tabindex="-1"></a><span class="in">  Xregr &lt;- x[, fit$jchosen]</span></span>
<span id="cb9-751"><a href="#cb9-751" aria-hidden="true" tabindex="-1"></a><span class="in">  datregr &lt;- data.frame(y, Xregr)</span></span>
<span id="cb9-752"><a href="#cb9-752" aria-hidden="true" tabindex="-1"></a><span class="in">  fitglm &lt;- glm(y ~ ., family = "poisson", data = datregr)</span></span>
<span id="cb9-753"><a href="#cb9-753" aria-hidden="true" tabindex="-1"></a><span class="in">  fit1 &lt;- glm(y ~ 1, family = "poisson", data = datregr)</span></span>
<span id="cb9-754"><a href="#cb9-754" aria-hidden="true" tabindex="-1"></a><span class="in">  modelscope &lt;- list(lower = formula(fit1), upper = formula(fitglm))</span></span>
<span id="cb9-755"><a href="#cb9-755" aria-hidden="true" tabindex="-1"></a><span class="in">  fitglmb &lt;- step(fit1, scope = modelscope, k = log(N), trace = 0, direction = "forward")</span></span>
<span id="cb9-756"><a href="#cb9-756" aria-hidden="true" tabindex="-1"></a><span class="in">  fitglmf &lt;- step(fitglm, scope = modelscope, k = log(N), trace = 0, direction = "backward")</span></span>
<span id="cb9-757"><a href="#cb9-757" aria-hidden="true" tabindex="-1"></a><span class="in">  if (BIC(fitglmf) &lt; BIC(fitglmb)) {fitglms &lt;- fitglmf} else {fitglms &lt;- fitglmb}</span></span>
<span id="cb9-758"><a href="#cb9-758" aria-hidden="true" tabindex="-1"></a><span class="in">  if (length(coef(fitglms)) &gt; 1)</span></span>
<span id="cb9-759"><a href="#cb9-759" aria-hidden="true" tabindex="-1"></a><span class="in">  {</span></span>
<span id="cb9-760"><a href="#cb9-760" aria-hidden="true" tabindex="-1"></a><span class="in">    vartype[j] &lt;- "Redundant"</span></span>
<span id="cb9-761"><a href="#cb9-761" aria-hidden="true" tabindex="-1"></a><span class="in">    text(rep(j, G), xmeans[, j], rep("R", G), col = 1:G)</span></span>
<span id="cb9-762"><a href="#cb9-762" aria-hidden="true" tabindex="-1"></a><span class="in">  } else {</span></span>
<span id="cb9-763"><a href="#cb9-763" aria-hidden="true" tabindex="-1"></a><span class="in">    vartype[j] &lt;- "Irrelevant"</span></span>
<span id="cb9-764"><a href="#cb9-764" aria-hidden="true" tabindex="-1"></a><span class="in">    text(rep(j, G), xmeans[, j], rep("I", G), col = 1:G)</span></span>
<span id="cb9-765"><a href="#cb9-765" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb9-766"><a href="#cb9-766" aria-hidden="true" tabindex="-1"></a><span class="in">  }</span></span>
<span id="cb9-767"><a href="#cb9-767" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb9-768"><a href="#cb9-768" aria-hidden="true" tabindex="-1"></a><span class="in">matplot(1:P, t(xmeans), col = 1:G, type = "c", lty = 1, add = TRUE)</span></span>
<span id="cb9-769"><a href="#cb9-769" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-770"><a href="#cb9-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-771"><a href="#cb9-771" aria-hidden="true" tabindex="-1"></a>The variables discriminate the clusters pacing strategies of</span>
<span id="cb9-772"><a href="#cb9-772" aria-hidden="true" tabindex="-1"></a>the runners are the number of laps covered during the last two thirds of</span>
<span id="cb9-773"><a href="#cb9-773" aria-hidden="true" tabindex="-1"></a>the race (except during the 13th and 23rd hours). The number of laps</span>
<span id="cb9-774"><a href="#cb9-774" aria-hidden="true" tabindex="-1"></a>covered during the first eight hours does not provide any additional</span>
<span id="cb9-775"><a href="#cb9-775" aria-hidden="true" tabindex="-1"></a>clustering information, and even no information at all for the number of laps</span>
<span id="cb9-776"><a href="#cb9-776" aria-hidden="true" tabindex="-1"></a>covered during the first hour.</span>
<span id="cb9-777"><a href="#cb9-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-778"><a href="#cb9-778" aria-hidden="true" tabindex="-1"></a>@fig-Running-result2 shows boxplots of the total number of loops covered by the runners in each of the</span>
<span id="cb9-779"><a href="#cb9-779" aria-hidden="true" tabindex="-1"></a>clusters.</span>
<span id="cb9-780"><a href="#cb9-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-781"><a href="#cb9-781" aria-hidden="true" tabindex="-1"></a><span class="in">```{r ARIrunning, echo = FALSE, message = FALSE, warning=FALSE, fig.align='center'}</span></span>
<span id="cb9-782"><a href="#cb9-782" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-Running-result2</span></span>
<span id="cb9-783"><a href="#cb9-783" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Number of loops covered by the runners of each clusters."</span></span>
<span id="cb9-784"><a href="#cb9-784" aria-hidden="true" tabindex="-1"></a><span class="in">tmp &lt;- sort(as.vector(rowSums(x)), decreasing = TRUE, index.return = TRUE)</span></span>
<span id="cb9-785"><a href="#cb9-785" aria-hidden="true" tabindex="-1"></a><span class="in">boxplot(rowSums(x) ~ map(Z), xlab = 'Cluster', ylab = 'Total number of laps')</span></span>
<span id="cb9-786"><a href="#cb9-786" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-787"><a href="#cb9-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-788"><a href="#cb9-788" aria-hidden="true" tabindex="-1"></a>Cluster 5  are clearly the most efficient runners. Looking at the running</span>
<span id="cb9-789"><a href="#cb9-789" aria-hidden="true" tabindex="-1"></a>strategy in @fig-Running-result1, we can see that they start as runners</span>
<span id="cb9-790"><a href="#cb9-790" aria-hidden="true" tabindex="-1"></a>of Cluster 1 and Cluster 2, but they managed to keep a constant pace on</span>
<span id="cb9-791"><a href="#cb9-791" aria-hidden="true" tabindex="-1"></a>the second part of the race, unlike those of the other two clusters</span>
<span id="cb9-792"><a href="#cb9-792" aria-hidden="true" tabindex="-1"></a>which faltered. Runners of Cluster 3 has covered the fewest number of</span>
<span id="cb9-793"><a href="#cb9-793" aria-hidden="true" tabindex="-1"></a>laps. Indeed, looking at their running strategy, we can see that most of these runners stop after the first third of</span>
<span id="cb9-794"><a href="#cb9-794" aria-hidden="true" tabindex="-1"></a>the race. Cluster 6 is relatively similar to Cluster 3, but runners</span>
<span id="cb9-795"><a href="#cb9-795" aria-hidden="true" tabindex="-1"></a>manage to continue running until half of the race is completed. Finally, Cluster 4 obtains</span>
<span id="cb9-796"><a href="#cb9-796" aria-hidden="true" tabindex="-1"></a>slightly better results than Cluster 6, starting more carefully, and</span>
<span id="cb9-797"><a href="#cb9-797" aria-hidden="true" tabindex="-1"></a>managing to run until the end of the race, even if the pace of the last</span>
<span id="cb9-798"><a href="#cb9-798" aria-hidden="true" tabindex="-1"></a>hours is not very constant.</span>
<span id="cb9-799"><a href="#cb9-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-800"><a href="#cb9-800" aria-hidden="true" tabindex="-1"></a><span class="fu"># Discussion</span></span>
<span id="cb9-801"><a href="#cb9-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-802"><a href="#cb9-802" aria-hidden="true" tabindex="-1"></a>A method for clustering and variable selection for multivariate count data has been proposed. The method is shown to give excellent performance on both simulated and real data examples. The method selects set of relevant variables for clustering and other variables are not selected if they are irrelevant or redundate for clustering purposes. </span>
<span id="cb9-803"><a href="#cb9-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-804"><a href="#cb9-804" aria-hidden="true" tabindex="-1"></a>The proposed method is shown to give interesting insights in the application domain, where some clusters members are shown to perform better overall to others and the benefits of constant (or near constant pacing) are shown. </span>
<span id="cb9-805"><a href="#cb9-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-806"><a href="#cb9-806" aria-hidden="true" tabindex="-1"></a>The level of variable selection is determined by the relative performance of the two models (as shown in @sec-interpretation) is compared. Alternative models to the Poisson GLM model which have greater flexibility could lead to a smaller set of selected variables than the proposed method achieves. This is a topic for future research. </span>
<span id="cb9-807"><a href="#cb9-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-808"><a href="#cb9-808" aria-hidden="true" tabindex="-1"></a>The proposed method is based on a conditionally independent Poisson mixture model for the selected variables. It could be argued that the conditional independence assumption is unrealistic in the application. @Hand2001 consider the implication of incorrectly assuming conditional independence in a classification setting and show that it can make the group membership probabilities over confident. Furthermore, in the conditional independent Poisson mixture model, the number of clusters can be upwardly biased, where extra clusters are included to model dependence in the data. The approach taken in the paper could be extended to use other multivariate count distributions, including multivariate distributions without the conditional independence assumption <span class="co">[</span><span class="ot">eg. @Karlis2018; @Karlis_2007; @Inouye_2017</span><span class="co">]</span>.</span>
<span id="cb9-809"><a href="#cb9-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-810"><a href="#cb9-810" aria-hidden="true" tabindex="-1"></a>The code for the proposed approach will be made available as an R package. </span>
<span id="cb9-811"><a href="#cb9-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-812"><a href="#cb9-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-813"><a href="#cb9-813" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowlegements</span></span>
<span id="cb9-814"><a href="#cb9-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-815"><a href="#cb9-815" aria-hidden="true" tabindex="-1"></a>This work was supported by the Science Foundation Ireland Insight</span>
<span id="cb9-816"><a href="#cb9-816" aria-hidden="true" tabindex="-1"></a>Research Centre (SFI/12/RC/2289_P2) and a visit to the Collegium --</span>
<span id="cb9-817"><a href="#cb9-817" aria-hidden="true" tabindex="-1"></a>Institut d'Études Avancées de Lyon.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>